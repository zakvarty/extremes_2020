[["index.html", "Lancaster University MATH562/482/582 Preface Acknowledgements", " Lancaster University MATH562/482/582 Lecturers: Clement Lee &amp; Zak Varty 2020/2021 Preface These notes are for both MATH562: Extreme Value Theory and MATH482/582: Assessing Financial Risk: Extreme Value Methods. The last chapter is for MATH482/582 only, and therefore not examinable for MATH562. As the course is to take place over five weeks, the suggested schedule is 1st week: Chapters 1 and 2 2nd week: Chapter 3 3rd week: Chapter 4 4th week: Chapters 5 and 6 5th week: Chapter 7 (for MATH482/582 only) Acknowledgements These notes were adapted from lecture notes that have been developed over the years by Prof. Jonathan Tawn, Dr Janet Heffernan, Dr Stuart Coles, Dr Emma Eastoe, and Dr Jenny Wadsworth. "],["intro.html", "1 Introduction 1.1 Role of Extreme Value Theory 1.2 Application areas of extreme value theory 1.3 What’s in the course for you?", " 1 Introduction 1.1 Role of Extreme Value Theory Consider a set of observations from independent and identically distributed (IID) random variables \\(X_1, \\ldots, X_n\\), with an unknown distribution function \\(F\\). Figure 1.1: Data density Suppose we want to estimate the tails of \\(F\\). Figure 1.1 illustrates the difficulties of estimating the tails of \\(F\\) accurately: most data are concentrated towards the centre of the distribution; by definition observations in the tails are scarce; estimates are often required beyond the sample maxima or minima; estimation is going to be difficult and will be driven strongly by the assumptions made. 1.1.1 Inadequacies of standard statistical approaches Why can standard modelling approaches not be used for estimating tails? We could fit a distributional model, e.g. \\(N(\\mu,\\sigma^2)\\), to all the data available and use this model to estimate tail probabilities; \\[\\begin{align*} \\hat{\\Pr}(X&gt;x)=1-\\hat{F}(x) =1-\\Phi\\left(\\frac{x-\\hat{\\mu}}{\\hat{\\sigma}}\\right) \\end{align*}\\] Problems with this include: estimates of \\(\\mu\\) and \\(\\sigma\\) are primarily driven by the central values of the data; assessment of model fit is driven by the central values; different models that fit the body of the data well can have very different extrapolations; if interest is only in the tails, why compromise the fit there by attempting to model the body of the distribution simultaneously? Extreme value theory provides procedures for tail estimation which are scientifically and statistically rational. It is also intrinsically about extrapolation. in all undergraduate courses we are taught not to trust any extrapolation (usually in regression contexts); in many applications extrapolations are necessary, so it seems sensible to develop procedures that have some scientific basis. 1.2 Application areas of extreme value theory In the course we will see a range of examples. In general, the most active areas of application of extreme value theory are (in order of activity): Environment (sea-levels, river-levels, wind speeds, temperatures, rainfall, pollution concentrations); Finance (financial markets, investment returns, insurance/re-insurance liabilities, actuarial lifetimes, portfolio risk) Internet traffic (file size, number and duration of connections); Reliability (weakest link principle) Athletics Statistical inference/tests. e.g. inference for \\(\\theta\\) when \\(X_i\\sim U(0, \\theta)\\) for \\(i=1, \\ldots, n\\). MLE \\(\\hat{\\theta}\\) satisfies: \\[\\begin{align*} \\hat{\\theta}=\\max(X_1, \\ldots, X_n). \\end{align*}\\] Asymptotic theory for sums. 1.2.1 Oxford and Worthing annual maxima temperatures The first data set, shown in Figure 1.2, gives measurements of annual maxima temperatures in degrees Fahrenheit from 1901 to 1980 in Oxford and Worthing, England. Figure 1.2: Annual maximum temperatures at Oxford and Worthing. 1.2.2 Oceanographic variables Figure 1.3 shows concurrent measurements of two oceanographic variables taken just off the Cornish coast at Newlyn: Surge (m): the non-tidal component of still water height, driven by local pressure systems and by wind; Wave height (m): also driven by wind and local meteorological conditions. In this example, dependence is due to reliance on common covariates. Figure 1.3: Surge and wave height at Newlyn. 1.2.3 FTSE Index THE FTSE100 index of the top UK 100 shares in an established index of share price with many investment portfolios set up to track its movement. Figure 1.4 shows data on the end of day value of the FTSE index on each week day over the period 1968 to 2001. Figure 1.4: Time-series of FTSE100 index. The general pattern in exponential growth, with some noticeable drops in value corresponding to the Stock Market crashes of Black Monday in October 1987, the Asian crash in 1999 and the reaction to 2001-09-11. The index value on a given day is highly related to the value the day before. However, what is interesting to the investor is how it changes from one day to the next, relative to its current value. These relative changes are given by the new variables \\[\\begin{align*} X_t=\\frac{FTSE_t-FTSE_{t-1}}{FTSE_{t-1}}\\quad\\mbox{ for each } t, t=2,3,\\ldots \\end{align*}\\] which we term the returns, plotted in Figure 1.5. The average value of the returns series is much more stable over time. Figure 1.5: Time-series of FTSE returns. 1.3 What’s in the course for you? This course will give you: an appreciation for a formal strategy for statistical modelling; insight into the issues associated with using asymptotically motivated models; an awareness of an area of probability theory rarely studied at undergraduate level; practice in applying standard statistical inference techniques in a new area of statistical application; and more background in the CLT and point / Poisson processes. "],["block-maxima.html", "2 Classical Theory of Maxima of IID Variables 2.1 Block Maxima 2.2 Domains of attraction 2.3 Inference for maxima of IID variables", " 2 Classical Theory of Maxima of IID Variables There are many ways to undertake tail estimation based on different characteristics of the sample which we may consider extreme. We will focus on univariate features. We start by assuming that the extremes are independent and identically distributed (IID). In this context we cover block maxima (Chapter 2) exceedances of some fixed level (Section 3.2) \\(r\\)-largest/smallest values (Section 3.3) We then move on to non-identically distributed data, and over covariate modelling (Section 4.2) process modelling (Section 4.3) Then we consider data which cannot be assumed to be independent, looking at maximum (Chapter 5) clustering of extremes (Chapter 6) Finally, we look at bivariate extremes through dependence measures (Chapter 7) 2.1 Block Maxima We will only present extreme value theory and methods for the upper tail. This is not restrictive due to the symmetry in the arguments. For example, let \\[\\begin{align*} M_{X,n}=\\max(X_1, \\ldots ,X_n) \\mbox{ and } m_{X,n}=\\min(X_1, \\ldots ,X_n). \\end{align*}\\] Then \\[\\begin{align*} m_{X,n}=-\\max(-X_1, \\ldots ,-X_n)=-M_{-X,n}, \\end{align*}\\] so all distributional results for minima can be derived from results for maxima. Notation: when it is clear we will drop the \\(X\\) subscript from \\(M_{X,n}\\), i.e. \\[\\begin{eqnarray*} M_{n}=\\max(X_1, \\ldots ,X_n). \\end{eqnarray*}\\] 2.1.1 Distributional Theory for \\(M_n\\) For the remainder of this section, we will focus on the distribution of sample maxima. Suppose that \\(X_1, \\ldots, X_n\\) is a sequence of IID random variables with distribution function \\(F\\). Then \\[\\begin{eqnarray*} \\Pr(M_n\\leq x) &amp; = &amp; \\Pr(X_1\\leq x,\\ldots, X_n\\leq x)\\\\ &amp; = &amp; \\Pr(X_1\\leq x)\\ldots \\Pr(X_n\\leq x)\\\\ &amp; = &amp; \\{F(x)\\}^n. \\end{eqnarray*}\\] If we are interested in \\(M_n\\) but \\(F\\) is unknown, this formula is not of any help. As we are often interested in the maximum of a large number of variables this suggests an approach to modelling \\(M_n\\) using an asymptotic argument. In particular, we may hope that a simple formulation may arise for the distribution of \\(M_n\\) as \\(n\\rightarrow \\infty\\), and that this formulation may not depend too strongly on the form of \\(F\\). However, \\(M_n\\rightarrow x^F\\) in probability as \\(n\\rightarrow \\infty\\), where \\[\\begin{eqnarray*} x^F=\\sup\\{x: F(x)&lt;1\\}, \\end{eqnarray*}\\] From this, \\(M_n\\) converges to the upper end point of \\(F\\). The asymptotic distribution of \\(M_n\\) is termed degenerate. This suggests that a bit more subtlety is required. Let us see what we can learn from the theory of sums. Aside on limit results for sums As previously let \\(X_1, \\ldots, X_n\\) be IID, but now we define \\(E(X_i)=\\mu\\), Var\\((X_i)=\\sigma^2&lt;\\infty\\). Let \\[\\begin{eqnarray*} \\overline{X}_n =\\frac{\\sum_{i=1}^n X_i}{n}. \\end{eqnarray*}\\] The weak law of large numbers says that \\[\\begin{eqnarray*} \\overline{X}_n \\rightarrow \\mu \\mbox{ (in probability) as }n\\rightarrow \\infty. \\end{eqnarray*}\\] Therefore the asymptotic distribution of \\(\\overline{X}_n\\) is degenerate. The Central Limit Theorem (CLT) overcomes this problem by including a linear normalisation so that for all fixed \\(x\\) \\[\\begin{eqnarray*} \\Pr\\left(\\frac{\\overline{X}_n-\\mu_n}{\\sigma_n}\\leq x\\right)\\rightarrow \\Phi(x) \\mbox{ as }n\\rightarrow \\infty, \\end{eqnarray*}\\] where \\(\\mu_n=E(\\overline{X}_n)=\\mu\\) , \\(\\sigma_n=\\sqrt{\\mbox{Var}(\\overline{X}_n)}=\\sigma/\\sqrt{n}\\) and \\(\\Phi\\) is the distribution function of a standard Normal random variable. CLT revision: the limit distribution is the same whatever the underlying distribution \\(F\\) (provided \\(\\sigma^2&lt;\\infty\\)); the normalisation gives the variable \\[\\begin{eqnarray*} n^{1/2}(\\overline{X}_n-\\mu)/\\sigma, \\end{eqnarray*}\\] i.e. normalisation blows up differences between \\(\\overline{X}_n\\) and \\(\\mu\\). the normalisation depends on the features of \\(F\\); the CLT motivates the model \\[\\begin{eqnarray*} \\overline{X}_n \\sim N(\\mu,\\sigma^2/n) \\end{eqnarray*}\\] as a model for finite \\(n\\). This is an asymptotically justified model for observations which are formed by taking sums. 2.1.2 Illustrative example: the maximum of Exponential variables We now return to maxima. Consider the maximum of Exponential\\((1)\\) variables \\(X_1,\\ldots,X_n\\). Is it possible to obtain a linear normalisation of \\(M_n\\) to give a non-degenerate limit distribution? First take the cumulative distribution function of the Exponential\\((1)\\) distribution: \\[\\begin{eqnarray*} F(x)=1-\\exp(-x) \\mbox{ for } x&gt;0. \\end{eqnarray*}\\] As \\(M_n\\rightarrow \\infty\\) we need to blow up the deviations of \\(M_n\\) from \\(\\infty\\). Try \\(M_n-\\log n\\). Then \\[\\begin{eqnarray*} \\Pr(M_n-\\log n\\leq x) &amp;= &amp;\\Pr(M_n\\leq x+\\log n)\\\\ &amp; = &amp; \\{F(x+\\log n)\\}^n\\\\ &amp; = &amp; \\{1-\\exp(-x-\\log n)\\}^n \\mbox{ for } x&gt;-\\log n\\\\ &amp; = &amp; \\{1-\\exp(-x)\\exp(-\\log n)\\}^n\\\\ &amp; = &amp; \\{1-\\exp(-x)/n\\}^n\\\\ &amp; \\rightarrow &amp; \\exp[-\\exp(-x)] \\mbox{ as }n \\rightarrow \\infty \\mbox{ for }-\\infty &lt; x&lt;\\infty. \\end{eqnarray*}\\] The last step uses the result \\((1+y/n)^n\\rightarrow \\exp(y)\\) as \\(n\\rightarrow \\infty\\) for any fixed \\(y\\). 2.1.3 Extremal Types Theorem (ETT) For subsequent use, we need to introduce the notion of an equivalence class of distributions, i.e. distributions of the same type If \\(F_1\\) and \\(F_2\\) are two distribution functions and there exist constants \\(a&gt;0\\) and \\(b\\) such that \\[\\begin{eqnarray*} F_2(ax+b)=F_1(x) \\mbox{ for all }x, \\end{eqnarray*}\\] then \\(F_1\\) and \\(F_2\\) are of the same type. So the two distributions are the same up to location and scale parameters. Examples: \\(N(\\mu_1,\\sigma_1^2)\\) and \\(N(\\mu_2,\\sigma_2^2)\\) are of the same type; \\(\\Gamma(\\alpha,\\beta)\\) is Gamma distribution with scale \\(\\beta\\) and shape \\(\\alpha\\), then \\(\\Gamma(\\alpha,\\beta_1)\\) and \\(\\Gamma(\\alpha,\\beta_2)\\) are of the same type; \\(\\Gamma(\\alpha_1,\\beta)\\) and \\(\\Gamma(\\alpha_2,\\beta)\\) are not of the same type. The following result can be viewed as an analogue to the CLT, for block maxima. Further details can be found in Leadbetter (1983). Theorem Extremal Types Theorem If there exist sequences of constants \\(a_n&gt;0\\) and \\(b_n\\), such that, as \\(n\\rightarrow \\infty\\) \\[\\begin{eqnarray*} \\Pr\\left(\\frac{M_n-b_n}{a_n}\\leq x\\right)\\rightarrow G(x) \\end{eqnarray*}\\] for some non-degenerate distribution \\(G\\), then \\(G\\) is of the same type as one of the following distributions: Gumbel \\[\\begin{eqnarray*} G(x)=\\exp\\{-\\exp(-x)\\} \\mbox{ for }-\\infty&lt;x&lt;\\infty; \\end{eqnarray*}\\] Fréchet \\[\\begin{eqnarray*} G(x)=\\left\\{ \\begin{array}{ll} 0 &amp; x\\leq 0\\\\ \\exp(-x^{-\\alpha}) &amp; x&gt;0, \\alpha&gt;0; \\end{array} \\right. \\end{eqnarray*}\\] Negative Weibull \\[\\begin{eqnarray*} G(x)=\\left\\{ \\begin{array}{ll} \\exp[-(-x)^{\\alpha}] &amp; x&lt;0, \\alpha&gt;0;\\\\ 1 &amp; x\\geq 0. \\end{array} \\right. \\end{eqnarray*}\\] 2.1.4 The Generalised Extreme Value (GEV) Distribution For statistical purposes it is inconvenient to work with three distinct classes of limiting distribution as in the ETT, so it is preferable to adopt a parametrisation which unifies these distributions. von Mises (1954) and Jenkinson (1955) derived the Generalised Extreme Value GEV\\((\\mu,\\sigma,\\xi)\\) distribution with distribution function \\[\\begin{eqnarray*} G(x) = \\exp\\left\\{-\\left[1+\\xi\\left(\\frac{x-\\mu}{\\sigma}\\right)\\right]_+^{-1/\\xi}\\right\\} \\end{eqnarray*}\\] where \\(x_+=\\max(x,0)\\) and \\(\\sigma&gt;0\\), so up to type the GEV distribution is \\[\\begin{eqnarray*} G(x)=\\exp\\left[-(1+\\xi x)_+^{-1/\\xi}\\right]. \\end{eqnarray*}\\] Gumbel corresponds to \\(\\xi=0\\) (taken as the limit \\(\\xi\\rightarrow0\\)); GEV\\((0,1,0)=\\) Gumbel; Fréchet corresponds to \\(\\xi&gt;0\\); GEV\\((1,\\alpha^{-1},\\alpha^{-1})=\\) Fréchet\\((\\alpha)\\); Negative Weibull corresponds to \\(\\xi&lt;0\\); GEV\\((-1,\\alpha^{-1},-\\alpha^{-1})=\\) Negative Weibull\\((\\alpha)\\). 2.1.5 Unified Extremal Types Theorem (UETT) Theorem If there exist sequences of constants \\(a_n&gt;0\\) and \\(b_n\\), such that, as \\(n\\rightarrow \\infty\\) \\[\\begin{eqnarray} \\Pr\\left(\\frac{M_n-b_n}{a_n}\\leq x\\right)\\rightarrow G(x) \\tag{2.1} \\end{eqnarray}\\] for some non-degenerate distribution \\(G\\), then \\(G\\) is of the same type as \\[\\begin{eqnarray*} G(x)=\\exp\\left[-(1+\\xi x)_+^{-1/\\xi}\\right] \\end{eqnarray*}\\] for some value of \\(\\xi\\). Notes on UETT: \\(\\xi\\) is termed the shape parameter (or tail index); \\(\\xi&gt;0\\) heavy upper tail, \\(\\xi=0\\) Exponential upper tail, \\(\\xi&lt;0\\) tail with finite upper limit; UETT does not guarantee the existence of a non-degenerate limit or say which type will arise (i.e. which \\(\\xi\\) value) when such a limit exists (it depends on \\(F\\), but only weakly); Unlike the CLT, the UETT does not tell us how to pick \\(a_n\\) and \\(b_n\\) (see Section 2.2); Unlike the CLT, the UETT has an infinite set of limit distributions (indexed by \\(\\xi\\)). 2.1.6 Moments of the GEV Suppose \\(Y\\sim\\) GEV\\((\\mu,\\sigma,\\xi)\\). Then the moments of the GEV are as follows (there is no need to derive these). If \\(r&gt;0\\) and \\(\\xi&gt;1/r\\) then \\[\\begin{eqnarray*} E(Y^r)=\\sum_{j=0}^r \\binom{r}{j} (\\mu-\\sigma/\\xi)^j(\\sigma/\\xi)^{r-j} \\Gamma(1-\\xi(r-j)) \\end{eqnarray*}\\] and \\(E(Y^r)=\\infty\\) if \\(\\xi\\geq1/r\\). So the expectation and variance are not always finite, in particular \\[\\begin{eqnarray*} E(Y)=\\mu+\\frac{\\sigma}{\\xi}[\\Gamma(1-\\xi)-1]\\mbox{ for }\\xi&lt;1 \\end{eqnarray*}\\] and Var\\((Y)=\\infty\\) if \\(\\xi\\geq 1/2\\). 2.1.7 Connections between CLT and UETT Is the UETT really much weaker than the CLT? UETT gives a family of limit distribution types, parametrised by \\(\\xi\\), the CLT gives a single type, the Normal distribution. The difference is really a matter of story-telling. The CLT has the condition that Var\\((X_i)&lt;\\infty\\) which is not required for the UETT. If this restriction is removed then the results look much more similar. If Var\\((X_i)=\\sigma^2&lt;\\infty\\), \\[\\begin{eqnarray*} \\Pr\\left(\\frac{n^{1/2}(\\overline{X}_n-\\mu)}{\\sigma}\\leq x\\right) \\rightarrow \\Phi(x) \\mbox{ as }n\\rightarrow \\infty, \\end{eqnarray*}\\] If Var\\((X_i)=\\infty\\), there exists \\(c&gt;\\frac{1}{2}\\) such that \\[\\begin{eqnarray*} \\Pr\\left(n^{1-c}\\overline{X}_n\\leq x\\right) \\rightarrow \\mbox{SSL}(c) \\mbox{ as }n\\rightarrow \\infty, \\end{eqnarray*}\\] where SSL is a Sum-Stable-Law1 distribution. So more generally, sums have limit laws that are parameterised by \\(c\\). 2.1.8 Pseudo Proofs Here we justify why, if a non-degenerate limit distribution exists, it has to be of the stated form. Proof of CLT For simplicity of presentation we take E\\((X_i)=0\\) and Var\\((X_i)=1\\). Let \\(S_n\\) denote the standardised variable \\(\\overline{X}_n\\), \\[\\begin{eqnarray*} S_n = n^{1/2}\\overline{X}_n=\\frac{1}{\\sqrt{n}}\\sum_{i=1}^n X_i, \\end{eqnarray*}\\] then E\\((S_n) = 0\\) and Var\\((S_n) =1\\). Let us assume that \\(S_n\\) converges in distribution to some random variable \\(Y\\) with an unknown distribution. That means that when \\(n\\) is large the distribution of \\(S_n\\) is well approximated by the distribution of \\(Y\\), or that the distribution of \\(\\sum_{i=1}^n X_i\\) is well approximated by the distribution of \\(\\sqrt{n}Y\\). We now think of the sum \\(\\sum_{i=1}^n X_i\\) as being the sum of sums. Specifically, let \\(k\\) be a fixed positive integer and define \\(r_n=\\mbox{integer part}(n/k)\\). Then, on ignoring the last little bit of the sum, \\[\\begin{eqnarray*} \\sum_{i=1}^n X_i \\approx \\sum_{j=1}^k \\sum_{i=r_{n}(j-1)+1}^{r_{n}j} X_i \\end{eqnarray*}\\] Note that the smaller sums are independent, because they are sums over different \\(X\\)’s. If \\(n\\) is large, so is \\(r_n\\), so each smaller sum has approximately the same distribution as \\(\\sqrt{r_n}Y\\). Now, letting \\(Y_1, \\ldots ,Y_k\\) be independent random variables each distributed as \\(Y\\), this implies that \\[\\begin{eqnarray*} \\sqrt{n}Y &amp; = &amp; \\sqrt{r_n}Y_1+ \\ldots +\\sqrt{r_n}Y_k\\\\ \\sqrt{k}Y &amp; = &amp; Y_1+ \\ldots +Y_k \\end{eqnarray*}\\] Thus the distribution of \\(Y\\) has to have the property that when we add independent random variables with the same distribution as \\(Y\\) we get (apart from scaling) the same distribution back. This is exactly the convolution property of the Normal distribution, and since the Normal distribution is the only distribution with finite variance with this property this shows at least heuristically why the limit distribution (if it exists) of the sum of IID random variables with finite variance has to be Normal. Proof of UETT Let us assume that \\((M_n-b_n)/a_n\\) converges in distribution to some random variable \\(Y\\) with an unknown distribution. That means that when \\(n\\) is large the distribution of \\((M_n-b_n)/a_n\\) is well approximated by the distribution of \\(Y\\), or that the distribution of \\(M_n\\) is well approximated by the distribution of \\(a_{n}Y+b_{n}\\). We now think of \\(M_n\\) as being the maximum of maxima. Specifically, let \\(k\\) be a fixed positive integer and define \\(r_n=\\mbox{integer part}(n/k)\\). \\[\\begin{eqnarray*} M_n \\approx \\max_{j=1,\\ldots, k} \\max(X_{r_{n}(j-1)+1}, \\ldots X_{r_{n}j}). \\end{eqnarray*}\\] Again, this requires us to ignore the last little bit of the max. These smaller maxima are independent, because they are maxima over different \\(X\\)’s. If \\(n\\) is large, so is \\(r_n\\), so each smaller maxima has approximately the same distribution as \\(a_{r_n}Y+b_{r_n}\\). Letting \\(Y_1, \\ldots ,Y_k\\) be independent random variables each distributed as \\(Y\\) then this implies \\[\\begin{eqnarray*} a_{n}Y+b_n &amp; = &amp; a_{r_n}\\max(Y_1, \\ldots ,Y_k)+b_{r_n}\\\\ \\left\\{ \\frac{a_{n}Y+b_{n}-b_{r_n}}{a_{r_n}} \\right\\} &amp; = &amp; \\max(Y_1, \\ldots ,Y_k) \\end{eqnarray*}\\] Thus the distribution of \\(Y\\) has to have the property that when we maximise independent random variables with the same distribution as \\(Y\\) we get the same distributional type back. This property can be written in terms of the distribution function as \\[\\begin{align} G(A_k x+B_k)=\\{G(x)\\}^k \\mbox{ for constants }A_k&gt;0 \\mbox { and }B_k \\end{align}\\] for all \\(k\\). This property is called max-stability. The GEV distribution is the only distribution which satisfies this max-stability property. This shows, at least heuristically, why the limit distribution (if it exists) of the maximum of IID random variables has to be GEV. 2.2 Domains of attraction In statistical applications we often pay little regard to the population distribution \\(F\\), but use the above theory to motivate the fitting of a GEV to block maxima \\(M_n\\). This reflects standard practice throughout statistical inference in which tests are based on the asymptotic normality of sample means without reference to the parent distribution. There is substantial probabilistic research in extreme value theory, a major concern of which has been characterising the domains of attraction for the extreme value limits, or in other words: given a limit distribution from the GEV class, characterise the set of distributions \\(F\\) for which the normalised \\(M_n\\) have that limit; or alternatively, given a distribution \\(F\\), find \\(a_n\\) and \\(b_n\\) such that a limit for \\(M_n\\) is obtained, and what is that limit? In full generality, this is a hard question. We focus on domains of attraction for random variables with absolutely continuous distributions. 2.2.1 Domains of attraction for random variables with absolutely continuous distributions Define the reciprocal hazard function \\(h\\) by \\[\\begin{eqnarray} h(x)=\\frac{1-F(x)}{f(x)} \\mbox{ for }x_F&lt;x&lt;x^F, \\tag{2.2} \\end{eqnarray}\\] where \\(f(x)\\) is the density function, \\(x_F\\) and \\(x^F\\) are the lower and upper end points of the distribution respectively.2 By a series of rearrangements of the distribution function, it can be shown that for each \\(u\\) and \\(x\\) there exists a \\(y\\) such \\(u\\leq y \\leq u+xh(u)\\) such that \\[\\begin{eqnarray} \\frac{1-F(u+xh(u))}{1-F(u)}=[1+h^{\\prime}(y)x]_+^{-1/h^{\\prime}(y)}. \\tag{2.3} \\end{eqnarray}\\] We suppose that \\(h^{\\prime}(y)\\rightarrow \\xi\\) as \\(y\\rightarrow x^F\\) (where \\(\\xi\\) is finite). Now fix \\(x\\) and let \\(u\\rightarrow x^F\\), and define \\(b_n\\) to be the \\(1-1/n\\) quantile, i.e. \\(1-F(b_n)=1/n\\), \\(a_n=h(b_n)\\), i.e. a function of the hazard in the extremes. Letting \\(u=b_n\\) in equation (2.3) gives \\[\\begin{eqnarray} n[1-F(a_n x+b_n)]\\rightarrow (1+\\xi x)_+^{-1/\\xi} \\mbox{ as }n\\rightarrow \\infty. \\tag{2.4} \\end{eqnarray}\\] As \\(-\\log(x)=-\\log[1-(1-x)]\\approx 1-x\\) as \\(x \\uparrow 1\\) then the left hand side of equation (2.4) is approximately \\(-n\\log F(a_n x+b_n)\\), so it follows that \\[\\begin{eqnarray*} \\{F(a_n x+b_n)\\}^n\\rightarrow\\exp[-(1+\\xi x)_+^{-1/\\xi}]. \\end{eqnarray*}\\] Thus the domains of attraction problem is simple: a GEV\\((0,1,\\xi)\\) type distribution is obtained as a limit if \\[\\begin{eqnarray*} h^{\\prime}(y)\\rightarrow \\xi \\mbox{ as }y\\rightarrow x^F, \\end{eqnarray*}\\] and we pick \\(a_n\\) and \\(b_n\\) by: \\[\\begin{eqnarray*} 1-F(b_n)=1/n \\mbox{ and }a_n=h(b_n). \\end{eqnarray*}\\] A Gumbel limit is obtained when the reciprocal hazard function is approximately constant for extreme values (this arises frequently in applications). 2.2.2 Examples of domains of attractions Exponential(1) \\(F(x)=1-\\exp(-x)\\), \\(f(x)=\\exp(-x)\\) and so \\(h(x)=1\\) for all \\(x&gt;0\\) (constant hazard). Then \\[\\begin{eqnarray*} h^{\\prime}(x)=0 \\mbox{ for all } x \\end{eqnarray*}\\] so \\(\\xi=0\\), \\(\\exp(-b_n)=1/n\\) so \\(b_n=\\log n\\) and \\(a_n=h(b_n)=1\\). Thus, \\[\\begin{eqnarray*} M_n-\\log n \\end{eqnarray*}\\] converges to a Gumbel distribution. Normal distribution For large \\(x\\) \\[\\begin{eqnarray*} 1-\\Phi(x)\\approx (2\\pi)^{-1/2}\\exp(-x^2/2)(x^{-1}-x^{-3}+3x^{-5}+ \\ldots ). \\end{eqnarray*}\\] It follows that \\(h(x)=x^{-1}-x^{-3}+ \\ldots\\), hence \\[\\begin{eqnarray*} h^{\\prime}(x)=-x^{-2}+3x^{-4}+ \\ldots \\end{eqnarray*}\\] so \\(\\xi=0\\) and \\(b_n\\) can be only expressed asymptotically as \\[\\begin{eqnarray*} b_n=(2\\log n)^{1/2}-\\frac{1}{2}(2\\log n)^{-1/2}[\\log(\\log n)+\\log(4\\pi)] \\mbox{ and } a_n=(2\\log n)^{-1/2}. \\end{eqnarray*}\\] Note that since \\(a_n\\rightarrow 0\\) it implies that \\(M_n\\sim b_n\\), i.e. \\(M_n \\sim (2\\log n)^{1/2}\\). Maxima of Normal variables grow very slowly, and deterministically! Distributions with regularly varying upper tails An important class of distributions have regularly varying tails, i.e. \\[\\begin{eqnarray*} \\frac{1-F(tx)}{1-F(t)}\\rightarrow x^{-\\alpha} \\mbox{ as } t\\rightarrow \\infty \\end{eqnarray*}\\] for fixed \\(x&gt;0\\) and \\(\\alpha&gt;0\\). Within this class are distributions with upper tails with \\[\\begin{eqnarray*} 1-F(x) \\sim c/x^{\\alpha} \\mbox{ as }x\\rightarrow \\infty, \\end{eqnarray*}\\] for \\(c&gt;0\\) and \\(\\alpha&gt;0\\). Examples include Pareto, Cauchy, \\(t\\) and \\(F\\) distributions. Then \\[\\begin{eqnarray*} f(x)\\sim \\alpha c x^{-\\alpha-1}, h(x)\\sim \\alpha^{-1}x \\mbox{ and } h^{\\prime}(x)\\sim \\alpha^{-1}\\mbox{ as }x \\rightarrow \\infty. \\end{eqnarray*}\\] It follows that \\(\\xi=\\alpha^{-1}\\), \\(b_n=(cn)^{1/\\alpha}\\) and \\(a_n=\\alpha^{-1}(cn)^{1/\\alpha}\\). A Fréchet\\((\\alpha)\\) type limit is obtained. 2.3 Inference for maxima of IID variables We now introduce statistical models based on the probabilistic ideas seen so far and explore some issues that arise when we wish to carry out statistical inference for these models. The fundamental premise in all statistical extreme value modelling is that we can approximate the distribution of extreme values by the limiting theoretical forms. The issue throughout is then to define extreme values in the above to be sufficiently extreme that the approximation by the limiting form is good! 2.3.1 Inference for the GEV distribution The GEV distribution is used to model data arising as block maxima. Suppose that \\(X_1,X_2,\\ldots\\) is an IID sequence of random variables having common distribution function \\(F\\), and \\(M_n=\\max(X_1,\\ldots,X_n)\\). According to the Unified Extremal Types Theorem, we assume that \\[\\begin{eqnarray*} M_n\\sim\\mbox{GEV}(\\mu,\\sigma,\\xi) \\end{eqnarray*}\\] Why should this be appropriate? We start by assuming the limit in the expression (2.1) to hold exactly for some finite \\(n\\) so that \\[\\begin{eqnarray} \\Pr\\left(\\frac{M_n-b_n}{a_n}\\leq x\\right) = \\exp\\left[-(1+\\xi x)_+^{-1/\\xi}\\right] \\tag{2.5} \\end{eqnarray}\\] Although the sequences \\(a_n\\) and \\(b_n\\) in the expression (2.1) depend on the original underlying distribution \\(F\\), for finite, fixed \\(n\\), they are just normalising constants so we can rewrite (2.5) as \\[\\begin{eqnarray*} \\Pr(M_n\\leq y) = \\exp\\left[-(1+\\xi x)_+^{-1/\\xi}\\right]\\mbox{ with }y=a_n x+b_n \\end{eqnarray*}\\] That is \\[\\begin{eqnarray*} \\Pr(M_n\\leq y) &amp;=&amp;\\exp\\left[-\\left\\{1+\\xi\\left(\\frac{y-b_n}{a_n}\\right)\\right\\}_+^{-1/\\xi}\\right]\\\\ &amp;=&amp;\\exp\\left[-\\left\\{1+\\xi\\left(\\frac{y-\\mu}{\\sigma}\\right)\\right\\}_+^{-1/\\xi}\\right]~~~\\sigma&gt;0. \\end{eqnarray*}\\] The key assumption we have made here is that of the expression (2.5). The validity of this assumption relies on: choice of \\(n\\): we need to construct \\(M_n\\) by taking the maximum of sufficiently many observations; flatness of \\(h&#39;\\): with \\(h\\) defined as in equation (2.2), this is determined by the original distribution \\(F\\). Maximum likelihood estimation for the GEV The parameters in the GEV can be estimated using any one of a number of inference methods, including maximum likelihood, Bayesian, and method of moments (and variant there of). We stick to the first method. Maximisation of the log-likelihood obtained from the GEV model with respect to parameters \\(\\boldsymbol\\theta = (\\mu,\\sigma,\\xi)&#39;\\) gives the global maximum likelihood estimate over the entire GEV class of models. Note that Numerical maximisation is required as there is no analytical solution. Parameter constraints are necessary to ensure a log-likelihood value of \\(-\\infty\\) for parameter combinations for which the observed data lie beyond an endpoint of the distribution. Then subject to conditions on \\(\\xi\\) (\\(\\xi&gt;-1/2\\)) given below, \\[\\begin{eqnarray*} \\hat{\\boldsymbol{\\theta}}(\\boldsymbol{X})\\sim\\mbox{MVN}\\left(\\boldsymbol{\\theta},{\\cal I}_E^{-1}(\\boldsymbol{\\theta})\\right) \\end{eqnarray*}\\] In practice, as \\(\\hat{\\boldsymbol{\\theta}}\\) is also unknown, the (expected or observed) information matrix is evaluated at its estimate \\(\\hat{\\boldsymbol{\\theta}}(\\boldsymbol{X})\\) using the observed information, so we can write \\[\\begin{eqnarray*} \\hat{\\boldsymbol{\\theta}}(\\boldsymbol{X})\\sim\\mbox{MVN}\\left(\\boldsymbol{\\theta},{\\cal I}_O^{-1}(\\boldsymbol{\\theta}(\\boldsymbol{x}))\\right), \\end{eqnarray*}\\] where \\({\\cal I}_O^{-1}(\\hat{\\boldsymbol{\\theta}})\\) is the inverse of the observed information matrix evaluated at the MLE. \\({\\cal I}_O^{-1}(\\hat{\\boldsymbol{\\theta}})\\) can be calculated analytically, however it is easier to use numerical differencing. This is the approach taken in these notes and in the course labs. Confidence intervals for the parameter values and for derived quantities follow from the approximate Normality of the MLE. Regularity of maximum likelihood estimates Potential difficulties can arise concerning the regularity conditions required for the maximum likelihood estimator to exhibit the usual asymptotic properties. These difficulties arise because the endpoints of the support of the distributions we consider are determined by the parameter values, so that standard asymptotic results are not automatically applicable. Smith (1985) gives the following results for the GEV: \\(-1/2&lt;\\xi\\): MLE regular despite parameters determining an endpoint; \\(\\sqrt{n}(\\hat\\xi-\\xi)\\rightarrow\\)N\\((0,C)\\); \\(-1&lt;\\xi&lt;-1/2\\): estimators can be obtained but are super-efficient; \\(n^{-\\xi}(\\hat\\xi-\\xi)\\rightarrow\\)SSL\\((\\xi)\\); \\(\\xi&lt;-1\\): the fitted end-point equals the largest observation. When \\(\\xi&lt;-1/2\\), the distributions have very short bounded upper tails. This is rarely encountered in practice, so the theoretical limitations of maximum likelihood methods rarely cause practical difficulties. Other inferential approaches Bayesian inference This is also feasible Implemented using MCMC Prior information of great value in situations where we have little information about extremes of variables See Stuart G. Coles and Powell (1996) for more details Moment based estimators Lack flexibility No facility for easy development of covariate models Unable to reflect uncertainty accurately via skew confidence intervals See Hosking, Wallis, and Wood (1985), Dekkers, Einmahl, and de Haan (1989) for more details Hill estimator Will see a popular estimator for \\(\\xi&gt;0\\) later in chapter 7 for finance 2.3.2 Example: fitting the GEV to temperature maxima Figure 2.1: Q-Q plot for GEV fits to Oxford and Worthing annual maximum temperature data sets. The GEV distribution seems an appropriate choice of model for the Oxford and Worthing temperature data since these are annual maxima data. We check the appropriatenes of this model choice using Q-Q plots shown in Figure 2.1. Q-Q plots emphasise model fit to observations in the tail of the distribution. The GEV parameter estimates for these data sets are as follows: Oxford Worthing \\(\\hat\\mu\\) 83.8 (0.52) 78.5 (0.39) \\(\\hat\\sigma\\) 4.3 (0.36) 3.1 (0.27) \\(\\hat\\xi\\) -0.29 (0.07) -0.11 (0.07) The estimated correlation matrices for \\(\\boldsymbol\\theta\\) are, for Oxford and Worthington respectively: \\[ \\begin{aligned} \\mbox{Corr}\\left(\\hat{\\boldsymbol{\\theta}}\\right)&amp;=\\left(\\begin{array}{lll} 1.00 &amp; 0.00 &amp; -0.37\\\\ 0.00 &amp; 1.00 &amp; -0.57\\\\ -0.37 &amp; -0.57 &amp; 1.00\\\\ \\end{array}\\right) ~~~\\mbox{and}~~~ \\left(\\begin{array}{rrr} 1.00 &amp; 0.24 &amp;-0.35 \\\\ 0.24 &amp; 1.00 &amp; -0.38\\\\ -0.35 &amp; -0.38 &amp; 1.00\\\\ \\end{array}\\right) \\end{aligned} \\] The 95% confidence intervals for the shape parameter \\(\\xi\\) based on the Normal approximation are \\((-0.42, -0.15)\\) and \\((-0.25, 0.03)\\) for Oxford and Worthing, respectively. Contrast these with the profile likelihood based 95% confidence intervals which reflect the skewness in the likelihood for this parameter: Figure 2.2: Profile log-likelihood functions for the GEV shape parameter \\(\\xi\\) for the Oxford and Worthing temperature data sets. The profile likelihood for \\(\\xi\\) is calculated by fixing the value of \\(\\xi=\\xi_0\\) and maximising the log-likelihood with respect to \\(\\mu\\) and \\(\\sigma\\). This process is repeated for a number of values of \\(\\xi_0\\). The maximised values of the log-likelihood give the profile log-likelihood for \\(\\xi\\). 2.3.3 Return levels and return periods The practical question to be addressed in most applications is: What is the probability of a given process giving a value which exceeds a given level \\(z\\) in a future time period? Or, equivalently, What level \\(z\\) ensures this probability is sufficiently small? This is often expressed using return levels and return periods: Return period of level \\(z\\): the expected waiting time until \\(z\\) is next exceeded; \\(T\\) year return level: the level for which the expected waiting time between exceedances is \\(T\\) years. In applications \\(T\\) is often 100 years yet only 5-30 years of data are available. For IID processes, return level and return periods correspond to quantiles and exceedance probabilities respectively. If \\[\\begin{eqnarray*} 1-F(z_p)=p \\end{eqnarray*}\\] then the return level \\(z_p\\) has return period \\(p^{-1}\\) observations. Return level estimation for the GEV The \\(1/p\\) return level \\(z_p\\) is the \\(1-p\\) quantile of the GEV distribution for \\(0&lt;p&lt;1\\). Substituting the maximum likelihood estimates of the GEV parameters into the quantile function for the GEV, we obtain maximum likelihood estimates of the \\(1/p\\) return level as: \\[\\begin{eqnarray} \\hat z_p= \\left\\{ \\begin{array}{ll} \\hat\\mu - \\frac{\\hat\\sigma}{\\hat\\xi}\\left[1-\\{-\\log(1-p)\\}^{-\\hat\\xi}\\right], &amp;\\mbox{ for } \\hat\\xi\\neq0,\\\\ \\hat\\mu-\\hat\\sigma\\log\\{-\\log(1-p)\\},&amp;\\mbox{ for } \\hat\\xi=0. \\end{array} \\right. \\tag{2.6} \\end{eqnarray}\\] To obtain the variance of the estimated return level, the delta method gives us \\[\\begin{eqnarray*} \\mbox{Var}(\\hat z_p) = \\nabla z_p&#39;V\\nabla z_p, \\end{eqnarray*}\\] where \\(V\\) is the variance-covariance matrix of \\((\\hat\\mu,\\hat\\sigma,\\hat\\xi)\\) and \\(\\nabla z_p\\) is the vector of first derivatives of \\(z_p\\) with respect to \\(\\mu,\\sigma\\) and \\(\\xi\\) respectively, evaluated at the MLE’s for these parameters. This expression is used to construct confidence intervals for \\(\\hat z_p\\) based on the approximate Normal distribution of \\(\\hat z_p\\). Such confidence intervals are symmetric by construction, as can be seen in Figure 2.3, which shows \\(\\hat z_p\\) against return period, with pointwise 95% confidence intervals. Empirical return levels are also shown for model validation. There are 80 points in each data set so the largest data point corresponds to the empirical 80 year return level. The smaller curvature in the Worthing return level plot is due to the value of the shape parameter, \\(\\xi\\), for Worthing being closer to zero. If \\(\\xi=0\\) then the return level curve is linear on this scale. Figure 2.3: Annual maximum return levels for the Oxford and Worthing temperature data sets. The Normal approximation to the distribution of maximum likelihood estimators may be poor when we are estimating return levels that correspond to long return periods. Profile likelihood based inferences can often give a more accurate representation of uncertainty. We obtain the profile log-likelihood for return level \\(z_p\\) as follows: re-parameterise the GEV model so that \\(z_p\\) is a model parameter, for example: \\[\\begin{eqnarray*} \\mu = z_p+\\frac{\\sigma}{\\xi}\\left[1-\\{-\\log(1-p)\\}^{-\\xi}\\right]. \\end{eqnarray*}\\] The log-likelihood is now a function of parameters \\((z_p,\\sigma,\\xi)\\). Fix \\(z_p=z_p^\\ast\\) and maximise the log-likelihood} with respect to the remaining parameters. Repeat step 2 for a number of values of \\(z_p^\\ast\\). The maximised values of the log-likelihood give the profile log-likelihood for \\(z_p\\). The skewness of the profile log-likelihood for return levels \\(z_p\\) reflects the greater uncertainty we have about higher return levels, where we have less information from the data. Figure 2.4: Profile log-likelihoods for the 20-year return level of the annual maxima for the Oxford and Worthing temperature datasets. The plots in Figure 2.4 show the profile log-likelihoods for the 20 year return level \\(z_{0.05}\\). These profile log-likelihoods are reasonably symmetric as we have 80 years’ worth of data so estimation of this quantile does not involve extrapolation. The plots in Figure 2.5 shows the profile log-likelihoods for the 200 year return level \\(z_{0.005}\\). These profile log-likelihoods are much more skewed than the profile log-likelihoods for the 20-year return level shown in Figure 2.4. Figure 2.5: Profile log-likelihoods for the 200-year return level of the annual maxima for the Oxford and Worthing temperature datasets. Pooling data for more efficient estimation Many environmental applications of extreme value methods have found that the assumption of a common shape parameter over suitably similar scenarios is often reasonable in practice. The shape parameter \\(\\xi\\) is the most difficult parameter of the GEV to estimate, and so if information can be pooled from different sources to estimate this parameter then considerable efficiency gains can be made. A justification for this approach is that the shape parameter is thought to describe an inherent feature of many processes. Changes in scenario that affect the size or variability of the observed values of the process do not change the shape characteristics of these observations. Of course, any assumption of common shape parameter must be supported by diagnostics such as formal likelihood ratio tests, as well as graphical model diagnostics for the pooled data sets, such as the Q-Q plots shown in Section 2.3.2. Examples of applications in which a common shape parameter is assumed include Buishand (1989); Smith (1989); S. G. Coles and Tawn (1990); S. G. Coles and Tawn (1996); Robinson and Tawn (1997); Heffernan and Tawn (2001); and Heffernan and Tawn (2003). Previously we fitted the GEV separately to the Oxford and Worthing data sets. These datasets both describe the same type of process i.e. temperature annual maxima and therefore it is appropriate to consider pooling data from both datasets to estimate the shape parameter jointly. Confidence intervals for \\(\\xi\\) overlap, suggesting that a common shape parameter may indeed be appropriate here. We assume independence between the variables at the two sites and use a Generalised Likelihood Ratio Test, comparing: Null Model: with common shape parameter; Alternative Model: with separate shape parameters. We obtain a deviance \\[\\begin{eqnarray*} D=2\\left\\{l_{\\mbox{sep}} - l_{\\mbox{pooled}}\\right\\} = 3.18 \\end{eqnarray*}\\] Under the Null Hypothesis, \\(D\\sim\\chi^2_1\\) and so we obtain a \\(p\\)-value of 0.07, and proceed with a common shape parameter. The separate and pooled model parameter estimates are: Oxford, Separate Worthing, Separate Oxford, Pooled Worthing, Pooled \\(\\hat\\mu\\) 83.8 (0.52) 78.5 (0.39) 83.5 (0.49) 78.6 (0.40) \\(\\hat\\sigma\\) 4.3 (0.36) 3.1 (0.27) 4.1 (0.31) 3.3 (0.30) \\(\\hat\\xi\\) -0.29 (0.07) -0.11 (0.07) -0.184 (0.049) Note particularly the effect on the standard errors of all the parameter estimates, most of which have reduced. Of course, here we have ignored any dependence between the annual maxima occurring in the same year at the two sites, so that we may have underestimated the uncertainty associated with our parameter estimates. Bibliography "],["alternative.html", "3 Alternative Characterisation and Improved Inferences 3.1 Point process characterisation 3.2 Threshold exceedances 3.3 The r-largest approach 3.4 Further reading", " 3 Alternative Characterisation and Improved Inferences So far, our interest has been in extreme values and asymptotically motivated models for tail estimation of \\(F\\) (the distribution of \\(X\\)). We have focused on asymptotic models for \\(G\\), the distribution of normalised \\(M_n\\). With regard to our objective this is not directly relevant theoretically, and likely to lead to inefficient statistical procedures - see Figure 3.1. Figure 3.1: Peaks over threshold v. block maxima. Motivation for extension of theory for maxima We want to learn about possible asymptotically motivated models for \\(F\\) arising from the limiting distribution \\(G\\). We will initially follow a heuristic approach making approximations that we will subsequently justify rigorously. Assume that the limit representation holds for some large \\(n\\); this means that \\[\\begin{eqnarray} \\{F(a_n x+b_n)\\}^n \\approx G(x)=\\exp\\left[-(1+\\xi x)_+^{-1/\\xi}\\right], \\tag{3.1} \\end{eqnarray}\\] for all \\(x\\) such that \\(a_n x+b_n\\) is near \\(x^F\\). Specifically assume that there is a threshold \\(u\\) near \\(x^F\\) such that equation (3.1) holds for all \\(x\\) with \\(a_n x+b_n&gt;u\\). From equation (3.1) it follows that \\[\\begin{eqnarray*} F(a_n x+ b_n) &amp; \\approx &amp; G^{1/n}(x) \\mbox{ for }a_n x+b_n&gt;u\\\\ F(y) &amp; \\approx &amp; G^{1/n}\\left( \\frac{y-b_n}{a_n}\\right) \\mbox{ for }y&gt;u.\\\\ \\end{eqnarray*}\\] The RHS here is still a GEV distribution due to the max-stability property. Hence for some \\(\\mu, \\sigma&gt;0, \\xi\\) parameters \\[\\begin{eqnarray*} F(y) &amp; \\approx &amp; \\exp\\left[-\\left\\{1+\\xi\\left(\\frac{y-\\mu}{\\sigma}\\right) \\right\\}_+^{-1/\\xi}\\right]\\mbox{ for }y&gt;u\\\\ F(y) &amp; \\approx &amp; 1-\\left[1+\\xi\\left(\\frac{y-\\mu}{\\sigma}\\right) \\right]_+^{-1/\\xi}\\mbox{ for }y&gt;u. \\end{eqnarray*}\\] This approximation is like a tail expansion for \\(F\\) near \\(x^F\\), with the tail corresponding to the upper tail of a GEV distribution. We will also focus on the distribution of the exceedances of a high threshold, i.e. \\(X\\,|\\,X&gt;u\\). Alternatively we may define the excess of threshold \\(u\\) as \\[\\begin{eqnarray*} Y_u=(X-u)_+. \\end{eqnarray*}\\] Then for \\(y\\geq 0\\) \\[\\begin{eqnarray} \\Pr(Y_u&gt;y\\,|\\,Y_u&gt;0) &amp; = &amp; \\Pr(X&gt;u+y\\,|\\, X&gt;u)\\nonumber\\\\ &amp; = &amp; \\frac{1-F(u+y)}{1-F(u)}\\nonumber\\\\ &amp; \\approx &amp; \\frac{[1+\\xi(u+y-\\mu)/\\sigma]_+^{-1/\\xi}} {[1+\\xi(u-\\mu)/\\sigma]_+^{-1/\\xi}}\\nonumber\\\\ &amp; = &amp; [1+\\xi y/\\sigma_u]_+^{-1/\\xi}, \\label{eqn:gpd} \\end{eqnarray}\\] where \\(\\sigma_u=\\sigma+\\xi(u-\\mu)\\). It follows that \\(Y_u\\,|\\,Y_u&gt;0\\sim \\mbox{GPD}(\\sigma_u,\\xi)\\), i.e. a generalised Pareto distribution with scale parameter \\(\\sigma_u\\) and shape parameter \\(\\xi\\). In general, if \\(Y \\sim \\mbox{GPD}(\\sigma, \\xi)\\), the cumulative distribution function is given by \\[ F(y) = \\left\\{\\begin{array}{ll} 1 - \\left(1 + \\xi\\frac{y}{\\sigma}\\right)^{-1/\\xi}, &amp; \\mbox{ for } \\xi \\neq 0, y&gt;0, 1 + \\xi\\frac{y}{\\sigma}&gt;0, \\\\ 1 - \\exp\\left(-\\frac{y}{\\sigma}\\right) &amp; \\mbox{ for } \\xi=0, y&gt;0. \\end{array}\\right. \\] 3.1 Point process characterisation Assume that \\(X_1, \\ldots ,X_n\\) are IID with distribution function \\(F\\); \\(F\\) is in the domain of attraction of \\(G\\) a GEV\\((0,1,\\xi)\\) distribution; the required norming constants are \\(a_n\\) and \\(b_n\\). We construct a sequence of point processes \\(P_1, P_2, \\ldots\\) on \\([0,1]\\times [0,\\infty)\\) by \\[\\begin{eqnarray*} P_n=\\left\\{\\left(\\frac{i}{n+1},\\frac{X_i-b_n}{a_n}\\right);i=1, \\ldots ,n \\right\\} \\end{eqnarray*}\\] and examine the limit process. Figure 3.2 shows examples of point process \\(P_n\\) for \\(n=5,10,100,500,1000,10000\\) respectively with \\(X_i\\sim\\) Exp(1). Figure 3.2: Example of point processes of various lengths. Notes on point process convergence: the limit process is non-degenerate as \\((M_n-b_n)/a_n\\) (which is the largest point in the \\(y\\)-axis) is non-degenerate; small points are normalised to the same value \\(b_l\\), with \\[\\begin{eqnarray*} b_l=\\lim_{n\\rightarrow \\infty}\\frac{x_F-b_n}{a_n}; \\end{eqnarray*}\\] large points of the process are retained in the limit process; if we can describe the limit process we have an asymptotically motivated model for all large values. 3.1.1 Aside on Poisson processes Consider a non-homogeneous Poisson process with intensity \\(\\lambda(x)\\) on the set \\(C\\). Let \\(N(A)\\) be the number of points of the Poisson process in the set \\(A\\subset C\\). The process then has the following properties: \\(N(A)\\) is a Poisson random variable for any \\(A\\subset C\\); The expected value of \\(N(A)\\) is obtained by integrating the intensity: \\[\\begin{eqnarray*} E(N(A))=\\int_{x\\in A} \\lambda(x)dx=\\Lambda(A), \\end{eqnarray*}\\] where \\(\\Lambda(A)\\) is termed the integrated intensity; \\(N(B_1), \\ldots ,N(B_k)\\) are independent random variables for any \\(k\\) and any disjoint sets \\(B_1, \\ldots ,B_k\\), with \\(B_i\\subset C\\) for all \\(i\\); The location of points of \\(A\\) have density \\(\\lambda(x)/\\Lambda(A), x\\in A\\). 3.1.2 The limiting point process Theorem Under the above condition on \\(P_n\\), on the set \\([0,1]\\times (b_l+\\epsilon,\\infty)\\), where \\(\\epsilon&gt;0\\), \\[\\begin{eqnarray*} P_n \\rightarrow P \\mbox{ as }n\\rightarrow \\infty, \\end{eqnarray*}\\] where \\(P\\) is a non-homogeneous Poisson process with intensity \\[\\begin{eqnarray*} \\lambda(t,x)=(1+\\xi x)_{+}^{-1-1/\\xi}. \\end{eqnarray*}\\] We will often be interested in sets of the form \\[\\begin{eqnarray*} B_y=[0,1]\\times (y,\\infty) \\end{eqnarray*}\\] where \\(y&gt;b_l\\), for which \\[\\begin{eqnarray*} \\Lambda(B_y) &amp; = &amp; \\Lambda([0,1]\\times (y,\\infty))\\\\ &amp; = &amp; \\int_{t=0}^{1} \\int_{x=y}^{\\infty}\\lambda(t,x)\\,dxdt\\\\ &amp; = &amp; \\int_{t=0}^{1} [-(1+\\xi x)_{+}^{-1/\\xi}]_{x=y}^{\\infty}\\,dt\\\\ &amp; = &amp; \\int_{t=0}^{1} (1+\\xi y)_{+}^{-1/\\xi} \\,dt\\\\ &amp; = &amp; (1+\\xi y)_{+}^{-1/\\xi}. \\end{eqnarray*}\\] 3.1.3 Proof of point process limit The strategy for proving that a point process \\(P_n\\) converges to a limit process \\(P\\) is as follows (Kallenberg 1983). Let \\(N_n(B)\\) and \\(N(B)\\) be the number of points of \\(P_n\\) and \\(P\\) respectively in set \\(B\\). Then for unions of sets of the form \\[\\begin{eqnarray*} B=(c_1,d_1]\\times \\ldots \\times (c_m,d_m] \\end{eqnarray*}\\] we have to show that as \\(n\\rightarrow \\infty\\) \\[\\begin{eqnarray*} E(N_n(B))\\rightarrow E(N(B)) \\end{eqnarray*}\\] and \\[\\begin{eqnarray*} \\Pr(N_n(B)=0)\\rightarrow \\Pr(N(B)=0). \\end{eqnarray*}\\] Proof of Limit Take \\(B_y=(0,1]\\times (y,\\infty)\\) The \\(i\\)-th point of \\(P_n\\) is in \\(B_y\\) if \\[\\begin{eqnarray*} \\frac{X_i-b_n}{a_n}&gt;y \\mbox{ i.e. if }X_i&gt;a_n y +b_n \\end{eqnarray*}\\] The probability of this is \\(1-F(a_n y +b_n)\\). Thus the expected number of such points is \\[\\begin{eqnarray*} E(N_n(B_y)) &amp; = &amp; n[1-F(a_n y +b_n)]\\\\ &amp; \\sim &amp; -\\log \\{F(a_n y +b_n)\\}^n\\\\ &amp; \\rightarrow &amp; -\\log G(y)\\\\ &amp; = &amp;(1+\\xi y)_{+}^{-1/\\xi}\\\\ &amp; = &amp; \\Lambda([0,1]\\times (y,\\infty))\\\\ &amp; = &amp; \\Lambda(B_y)\\\\ &amp; = &amp; E(N(B_y)). \\end{eqnarray*}\\] Now consider the event \\(N_n(B_y)=0\\). This can be expressed as \\[\\begin{eqnarray*} \\{N_n(B_y)=0\\}= \\left\\{\\frac{X_i-b_n}{a_n}\\leq y ~~\\forall i=1, \\ldots ,n\\right\\} =\\left\\{X_i \\leq a_ny+b_n ~~\\forall i=1, \\ldots ,n\\right\\} \\end{eqnarray*}\\] So \\[\\begin{eqnarray*} \\Pr(N_n(B_y)=0) &amp; = &amp; \\{F(a_n y+b_n)\\}^n\\\\ &amp; \\rightarrow &amp; G(y)\\\\ &amp; = &amp; \\exp[-(1+\\xi y)_{+}^{-1/\\xi}]\\\\ &amp; = &amp; \\exp[-\\Lambda(B_y)]\\\\ &amp; = &amp; \\Pr(N(B_y)=0). \\end{eqnarray*}\\] 3.1.4 The value of the Poisson process limit The result shows that the behaviour of all large values is determined (asymptotically) by the \\(a_n, b_n\\) and \\(\\xi\\) characteristics (just as for the maximum); if we use all threshold exceedances for statistical inference we have more data to use for inference but the same number of parameters to estimate as using maxima, this suggests potential efficiency gains; the likelihood for a Poisson process observed on \\([0,1]\\times (u,\\infty)\\) applied to exceedances \\((t_1, x_1), \\ldots ,(t_{n_u},x_{n_u})\\}\\) of \\(u\\) is \\[\\begin{eqnarray} \\prod_{i=1}^{n_{u}}\\lambda(t_i,x_i) \\times \\exp\\{-\\Lambda([0,1]\\times (u,\\infty))\\}. \\tag{3.2} \\end{eqnarray}\\] See Smith (1989) for statistical details of the point process approach. 3.2 Threshold exceedances An important consequence of the Poisson process result is that it provides an asymptotic motivation for the generalised Pareto distribution as a conditional model for excesses of a high threshold. We focus on points of the process \\(P_n\\) that are large i.e. above a threshold) and look at the distribution of exceedances of this level. For any fixed \\(v&gt;b_l\\), let \\[\\begin{eqnarray*} u_n(v)=a_n v+b_n, \\end{eqnarray*}\\] then \\(u_n(v)\\rightarrow x^F\\), and let \\(x&gt;0\\) then \\[\\begin{align*} &amp;\\Pr(X_i&gt;a_n x+u_n(v)\\,|\\, X_i&gt;u_n(v))\\\\ &amp;\\quad=\\Pr\\left(\\frac{X_i-b_n}{a_n}&gt;x+v \\,\\left |\\,\\frac{X_i-b_n}{a_n}&gt;v\\right. \\right)\\\\ &amp;\\quad=\\Pr\\left(\\mbox{a given point in } P_n&gt;x+v \\,|\\, \\mbox{a given point in } P_n&gt;v\\right)\\\\ &amp;\\quad \\rightarrow \\Pr\\left(\\mbox{a given point in } P&gt;x+v \\,|\\, \\mbox{a given point in } P&gt;v\\right)\\\\ &amp;\\quad= \\frac{\\Lambda(B_{x+v})}{\\Lambda(B_{v})}\\\\ &amp;\\quad= \\left[1+\\xi\\frac{x}{1+\\xi v}\\right]_+^{-1/\\xi}\\\\ &amp;\\quad= \\left[1+\\xi\\frac{x}{\\sigma_v}\\right]_+^{-1/\\xi}, \\end{align*}\\] where \\(\\sigma_v=1+\\xi v\\). Hence the limiting distribution for a scaled excess \\[\\begin{eqnarray*} \\frac{[X_i-u_n(v)]_+}{a_n}\\,|\\,X_i&gt;u_n(v) \\end{eqnarray*}\\] follows a generalised Pareto distribution, GPD\\((\\sigma_v,\\xi)\\). Threshold Stability: Furthermore, this limit distribution holds whatever value of \\(v\\), provided \\(v&gt;b_l\\). Hence there is stability of the form of the limit distribution with respect to the level of the threshold, provided the threshold is large enough for the Poisson process limit to be achieved. 3.2.1 Practical implications of limit This limiting result motivates the use of the GPD as a model for excess values over a threshold. This requires fixing \\(u_n(v)\\) at a high threshold, absorbing \\(a_n\\) into the scale parameter and taking the limit as an equality for all \\(x&gt;0\\). Let \\(Y_u\\) be the excess variable over threshold \\(u\\): \\[\\begin{eqnarray*} Y_u =(X-u)_+ \\end{eqnarray*}\\] then for \\(y&gt;0\\) \\[\\begin{eqnarray*} \\Pr(Y_u\\leq y\\,|\\,Y_u&gt;0)=1-(1+\\xi y/\\sigma_u)_+^{-1/\\xi} \\end{eqnarray*}\\] so \\(Y_u\\,|Y_u&gt;0\\sim \\mbox{GPD}(\\sigma_u,\\xi)\\). We now have a model for the excess variable, conditional on having observed an excess. To undo this conditioning and obtain a model for the original variable \\(X\\) we model the rate parameter \\[\\begin{eqnarray*} \\phi_u = \\Pr(Y_u&gt;0) = \\Pr(X&gt;u). \\end{eqnarray*}\\] In the IID case, it follows from the Poisson process limit that the estimate for this is simply the proportion of the data which exceed \\(u\\) \\[\\begin{eqnarray*} \\phi_u = \\frac{n_u}{n} \\end{eqnarray*}\\] where \\(n_u=\\sum_{i=1}^n I[y_t&gt;u]\\) and \\(I[y_t&gt;u]\\) is the indicator variable taking the value 1 if the observation is an exceedance. 3.2.2 Properties of GPD Influence of shape parameter As with the GEV, the parameter \\(\\xi\\) has a substantial impact on the upper tail. \\(\\xi&gt;0\\) is heavy tailed; \\(\\xi=0\\) is the Exponential distribution with mean \\(\\sigma_u\\); \\(\\xi&lt;0\\) is short tailed, with upper bound. Moments For \\(r&lt;1/\\xi\\) \\[\\begin{eqnarray*} E(Y_u^r\\,|\\,Y_u&gt;0)=\\frac{\\sigma_u}{\\xi}\\sum_{j=0}^r\\binom{r}{j} \\frac{(-1)^{j+1}}{(1-\\xi j)} \\end{eqnarray*}\\] otherwise \\[\\begin{eqnarray*} E(Y_u^r\\,|\\,Y_u&gt;0)=\\infty. \\end{eqnarray*}\\] More details of the GPD are given by Davison and Smith (1990). 3.2.3 Inference for the generalised Pareto distribution The generalised Pareto distribution (GPD) is used to model data arising as independent threshold exceedances. We assume that excesses \\(Y_{u,1},\\ldots,Y_{u,n}\\) of a high threshold \\(u\\) follow GPD(\\(\\sigma,\\xi\\)) so that \\[\\begin{eqnarray*} \\Pr(Y_u&lt;y\\,|\\,Y_u&gt;0)=1-\\left\\{1+\\xi\\left(\\frac{y}{\\sigma_u}\\right)\\right\\}_+^{-1/\\xi}~~~y&gt;0. \\end{eqnarray*}\\] Possibly the most important issue in statistical modelling of threshold exceedance data is the choice of threshold \\(u\\). Using as low a threshold as possible maximises the amount of data used, making the statistical inference more efficient; If too low a threshold is used, the GPD may not fit well as the asymptotic argument motivating its appropriateness may not apply. A threshold choice based on the observed sample is required to balance these two opposing demands. We can exploit some properties of the GPD to help us with threshold selection. Threshold stability This property states that if the random variable \\(Y_u\\) satisfies \\[\\begin{eqnarray} Y_u\\,|\\,Y_u&gt;0\\sim\\mbox{ GPD}(\\sigma_u,\\xi)\\mbox{ for some threshold }u \\tag{3.3} \\end{eqnarray}\\] then for any higher threshold \\(v\\ge u\\) \\[\\begin{eqnarray*} Y_v\\,|\\,Y_v&gt;0 \\sim\\mbox{ GPD}(\\sigma_u+\\xi(v-u),\\xi) \\end{eqnarray*}\\] so that \\(\\xi\\) is constant with threshold but \\(\\sigma_v=\\sigma_u+\\xi(v-u)\\) is not. Linear mean excess When \\(Y_u\\) satisfies (3.3), the mean excess over \\(v&gt;u\\) is \\[\\begin{eqnarray} E(Y_u | Y_u&gt;0) = \\{\\sigma_u + \\xi(v-u)\\}/(1-\\xi),\\mbox{ for }\\xi&lt;1. \\end{eqnarray}\\] Thus the shape parameter is invariant to threshold and \\(E(Y_u| Y_u&gt;0)\\) is linear in \\(v\\) with gradient \\(\\xi/(1-\\xi)\\). Formal algorithms that use within-sample characteristics to select a threshold by minimising the mean square error of the estimated shape parameter, under certain assumptions about the second order tail structure, have been proposed (Danielsson and Vries 1997; Guillou and Hall 2001). Threshold selection diagnostics: Mean excess (or residual life) plot The sample mean excess over a threshold is plotted against the threshold, for a range of threshold values. If the random variable follows a generalised Pareto distribution over a threshold \\(u\\) then the relationship shown by the plot should be a straight line above \\(u\\) with gradient \\(\\xi/(1-\\xi)\\). Confidence intervals are added to each of these plots using the approximate Normality of sample means. Threshold selection diagnostics: Parameter stability plot We re-parameterise the threshold dependent scale parameter \\(\\sigma_v\\) given above as follows: \\[\\begin{eqnarray} \\sigma^\\ast=\\sigma_v-\\xi v \\end{eqnarray}\\] Now \\(\\sigma^\\ast\\) does not depend on the choice of \\(v\\), provided the original threshold for fitting (\\(u\\)) is high enough. The estimated parameters \\(\\hat\\sigma^\\ast\\) and \\(\\hat\\xi\\) are plotted against the threshold used for estimation. If the random variable follows a generalised Pareto distribution over a threshold \\(u\\) then both parameters \\((\\sigma^\\ast,\\xi)\\) will be constant for exceedances of all thresholds \\(v\\) greater than \\(u\\). The selected threshold corresponds to the lowest value for which the described properties are achieved. Again, confidence intervals are added to this plot using the approximate Normality of the maximum likelihood estimates of the parameters. 3.2.4 Example: fitting the GPD to Newlyn data Note: The points made in Section 2.3.1 concerning maximum likelihood for the GEV distribution all apply equally to the use of maximum likelihood to fit the GPD to threshold exceedance data. In order to fit the GPD to the Newlyn wave and surge data, we must first choose an appropriate threshold. The mean residual life plots for these data sets are shown in Figure 3.3. These plots suggest a threshold of at least 0.05 for the surge and at least 1.5 for the waves. Figure 3.3: Mean excess plots for the Newlyn surge and wave data sets. Parameter stability plots for the surge variable are shown in Figure 3.4, and for the wave height variable in Figure 3.5. For surge, the parameter stability plots suggest a threshold of at least 0.1. For the wave variable, the parameter stability plots suggest a threshold of at least 2. Figure 3.4: Parameter stability plots for the Newlyn surge variable. Figure 3.5: Parameter stability plots for the Newlyn wave variable. We proceed with thresholds of 0.1 and 2 for the surge and wave respectively. These correspond to the 0.64 and 0.353 quantiles of these variables respectively. Parameter estimates for the GPD fitted at these thresholds are: Surge Wave \\(\\hat\\sigma\\) 0.12 (0.005) 1.91 (0.061) \\(\\hat\\xi\\) -0.10 (0.026) -0.165 (0.022) Corr\\((\\hat\\sigma,\\hat\\xi)\\) -0.73 -0.80 Both of these distributions are estimated to have a finite upper endpoint. Finally, Figure 3.6 displays contours of the log-likelihood surface showing the negative dependence between these parameters. Figure 3.6: Log-likelihood surfaces for the GP parameter for Newlyn surge (left) and wave (right) data. 3.2.5 Q-Q plot &amp; probability integral transform One common diagnostic plot is the Q-Q plot, which shows the empirical quantiles (according to actual data) against the expected quantiles (according to the model). When fitting the GEV or the GPD, it is possible that a lot of the plotting space is taken up by the few largest values. The plot might be clearer on a different scale. Transforming scales The probability integral transform (PIT) tells us that if \\(X\\sim F\\) then \\[\\begin{eqnarray*} U=F(X)\\sim \\mbox{Unif}(0,1) \\end{eqnarray*}\\] and if \\(U \\sim \\mbox{Unif}(0,1)\\) then \\[\\begin{eqnarray*} Y=G^{-1}(U) \\sim G. \\end{eqnarray*}\\] Putting these two together, we get for \\(Y_u=(X-u)_+ \\sim\\) GPD(\\(\\sigma_u, \\xi\\)) \\[\\begin{eqnarray*} \\frac{1}{\\xi}\\log\\left[1+\\xi\\left(\\frac{X-u}{\\sigma_u}\\right)\\right] \\sim \\mbox{Exp}(1). \\end{eqnarray*}\\] This leads to a Q-Q plot on a transformed scale, by computing the empirical quantiles using the expression on the left hand side (with \\(X\\) substituted by the data points), and the theoretical quantiles according to the Exp\\((1)\\) distribution. 3.2.6 Return levels &amp; return periods To estimate the return level \\(x_p\\) corresponding to return periods \\(1/p\\), we need to solve \\[ 1 - F(x_p) = p \\,. \\] For a given threshold \\(u\\), assume that \\(\\phi_u = \\Pr(X&gt;u)\\). For practical interest, we assume that \\(p\\) is much smaller than \\(\\phi_u\\). As \\(X-u|X&gt;u \\sim \\mbox{GPD}(\\sigma_u, \\xi)\\), we have \\[\\begin{eqnarray*} \\Pr(X&gt; x_p) &amp;=&amp; \\Pr(X&gt; x_p |X&gt;u) \\Pr(X&gt;u) = p \\\\ &amp;=&amp; \\left\\{1 + \\xi\\frac{x_p-u}{\\sigma}\\right\\}_+^{-1/\\xi} \\phi_u = p \\end{eqnarray*}\\] leading to \\[ x_p = \\left\\{\\begin{array}{ll} u + \\frac{\\sigma_u}{\\xi}\\left\\{\\left(\\frac{p}{\\phi_u}\\right)^{-\\xi} - 1\\right\\} &amp; \\mbox{ for } \\xi \\neq 0 \\\\ u -\\sigma \\log\\left(\\frac{p}{\\phi_u}\\right) &amp; \\mbox{ for } \\xi = 0 \\,. \\end{array}\\right. \\] 3.3 The r-largest approach This technique pre-dates the point process approach to inference but can be derived as a special case of it. Given a sequence of IID variables \\(X_1,\\ldots ,X_n\\) the block maximum method is based on the limiting distribution of \\[\\begin{eqnarray*} \\frac{M_n - b_n}{a_n}, \\end{eqnarray*}\\] the normalised maximum order statistic. We have seen how the GEV distribution can be fitted to such order statistics. The so-called \\(r\\)-largest approach allows us to estimate the parameters of the GEV using more than just the single largest observation from each block of data. Using more information generally leads to more efficient estimates. Defining \\(M_n^{(i)}\\) to be the \\(i\\)th largest order statistic, we obtain the limiting joint distribution of \\[\\begin{eqnarray*} \\left( \\frac{M_n^{(1)} - b_n}{a_n}, \\frac{M_n^{(2)} - b_n}{a_n},\\ldots, \\frac{M_n^{(r)} - b_n}{a_n} \\right) \\end{eqnarray*}\\] for some choice of \\(r\\). Setting \\(u = M_n^{(r)}\\) in (3.2) gives the following likelihood for the parameters of the GEV: \\[\\begin{eqnarray*} L(\\theta) = \\exp\\left\\{-\\left(1 + \\xi\\frac{M_n^{(r)} - \\mu}{\\sigma}\\right)_{+}^{-1/\\xi}\\right\\} \\prod_{i=1}^r\\sigma^{-1}\\left(1 + \\xi \\frac{M_n^{(i)} - \\mu}{\\sigma}\\right)_{+}^{-1/\\xi - 1}. \\end{eqnarray*}\\] There is a choice of how many order statistics per block should be used and this choice invokes the usual bias/variance trade-off: too many order statistics (\\(r\\) too large) results in bias due to the invalidity of the asymptotics; too few order statistics incurs large sampling variability. The issue is the same as that of threshold choice in the point process model. A practical approach is to fit using a range of values of \\(r\\) and proceed using a choice of \\(r\\) for which estimates are reasonable stable. Note that using the \\(r\\)-largest method with \\(r=1\\) is equivalent to the GEV annual maxima method. 3.4 Further reading In addition to the threshold selection diagnostics presented, there have been models and methods addressing the uncertainty around the threhold \\(u\\) and attempting to model it as a parameter. One common approach is the use of a mixture distribution; see, for example, Scarrott and MacDonald (2012), Behrens, Lopes, and Gamerman (2004), Gamerman and Lopes (2006), do Nascimento, Gamerman, and Lopes (2012), Frigessi, Haug, and Rue (2002), Carreau and Bengio (2009), MacDonald et al. (2011), Tancredi, Anderson, and O’Hagan (2006), Zhao et al. (2010), So and Chan (2014), and Hu and Scarrott (2018). Bibliography "],["non-iid.html", "4 Extremes of Non-identically Distributed Variables 4.1 Examples of non-identically distributed processes 4.2 Existing methods for non-identically distributed processes 4.3 Pre-processing method 4.4 Analysis of Reading ozone data 4.5 Further reading", " 4 Extremes of Non-identically Distributed Variables Though it is possible to study the asymptotics of maxima of processes with specified forms of non-identically distributed variables, the results are generally too specific to be of use in modelling data for which the form of departure from identically distributed is unknown. Since many interesting data sets are in fact non-identically distributed the question of how to produce good models in this situation is an interesting and important one. We begin by examining some examples of non-identically distributed processes. We shall then discuss some of the existing ideas on how to model such processes, before introducing an alternative approach which allows us to use information in the entire process to define the extremes. 4.1 Examples of non-identically distributed processes We look at two examples of non-identically distributed processes; Simulated data - where we can specify different kinds of marginal distribution; Reading ozone data set. We shall discuss what it means for a process to have non-identically distributed margins and some of the forms that this may take. These forms of non-identical distribution are specifically chosen to lead into the modelling approach discussed in Section . Trend in mean level The simplest form of non-identical marginal distribution is to have a trend in the mean (average) of a process. Here we show linear and cyclic (or seasonal) trends in Figure 4.1. Figure 4.1: Trend in mean. Trend in variability Alternatively, we might instead observe some form of trend in the variability of the process. Again the examples we show in Figure 4.2 have linear and cyclic trends in variance. Figure 4.2: Trend in variance. Notation Let \\(\\{Y_t\\}\\) be a non-identically distributed process and let \\(\\{Z_t\\}\\) be some IID process. We can express trends in the mean and variance of \\(\\{Y_t\\}\\) as \\[\\begin{eqnarray} Y_t = Z_t + \\alpha(t); \\tag{4.1} \\end{eqnarray}\\] and \\[\\begin{eqnarray} Y_t = \\beta(t) Z_t; \\tag{4.2} \\end{eqnarray}\\] where \\(\\alpha(t)\\) and \\(\\beta(t)&gt;0\\) are functions of time. Later we see that these can also be functions of covariates which may or may not vary in time. Note that in any practical situation we would observe only the data \\(\\{Y_t\\}\\) and covariates. Clearly expressions (4.1) and (4.2) can be combined if the process \\(\\{Y_t\\}\\) has trends in both mean and variance. In fact, we use these expressions to study the extremes of the observed process \\(\\{Y_t\\}\\) in Section 4.3. By defining the non-identically distributed process as a function of a sequence of IID random variables and a deterministic trend it becomes clear how to simulate the processes with trends in mean and/or variance. First simulate the IID sequence, the examples here use standard Gumbel random variables. Now add the the trend using equations (4.1) and (4.2) e.g. for a linear trend in mean the required mean function is \\(\\alpha(t)=\\alpha_0+\\alpha_1 t\\). The simplest method of simulating of the IID sequence is to use the probability integral transform on an IID sequence of standard uniform random variables. 4.1.1 Reading ozone data Surface-level ozone (O\\(_3\\)) levels measured at a site in central Reading are shown in Figure 4.3. Data are daily maxima of hourly observations and are measured in \\(\\mu\\)gm\\(^{-3}\\). The trends in this data are not so obvious, but there is clearly seasonality - in this case an annual cycle. Figure 4.3: Daily maximum ozone in central Reading. Why is treating a non-identically distributed process as if it were identically distributed unreasonable? From a statistical point of view, we would be violating the identically distributed part of the IID assumption which will lead to bias in parameter estimates. From a scientific point of view we would lose vital information about the underlying mechanisms which generate the data which could otherwise be incorporated in a more realistic model. It is always important to bear in mind what we want to use the model for - forecasting (prediction), an understanding of the mechanisms driving the process or identification of trends. All of these either benefit from or require a model which incorporates non-identical margins. Scientific notes on Ozone As discussed above, it is important to have as good an understanding of the scientific background to your data set as possible and to fully understand the uses of a statistical model. Interesting questions posed in the analysis of ozone include the prediction of high levels of ozone, for public health warnings, and the identification of long-term trends, to assess the impact of legislation to reduce pollution. Ozone is a secondary pollutant, meaning it is formed in the atmosphere, from precursors (chemicals such as NO\\(_\\mathrm{X}\\) and VOC’s) by a series of mostly photochemical (light driven) reactions. Thus meteorological conditions strongly influence the levels of ozone observed. Specifically, ozone levels are high in warm, sunny and still conditions. 4.2 Existing methods for non-identically distributed processes Existing methods for modelling non-identically distributed processes focus on fitting models with covariates. Specifically, the block maxima, \\(r\\)-largest and threshold exceedance models introduced for IID data are maintained, and the non-identical margins are modelled directly through the model parameters. Covariate models are easily fitted under the likelihood framework and the covariates in the best fitting models are selected using standard methods, e.g. using forward selection and comparing nested models with the likelihood ratio test. 4.2.1 Models for block maxima and \\(r\\)-largest This fitting is straightforward to do and involves iterating through the following steps: propose linear covariate models for the appropriate parameters of the GEV (the covariate could be time, but could be other explanatory variables depending on the application); choose a link function for these parameters; fit the proposed models using maximum-likelihood; use likelihood methods such as likelihood ratio tests for model selection; check model fit using standard diagnostic tools such as return level and Q-Q plots. Examples of possible models are a linear time trend in the mean or a linear time trend in the scale parameter, using the log-link function in the latter: \\[\\begin{eqnarray*} \\mathrm{GEV}(\\mu_0+\\mu_1t,\\sigma,\\xi);~~~\\mathrm{GEV}(\\mu,\\exp\\{\\sigma_0+\\sigma_1t\\},\\xi) \\end{eqnarray*}\\] 4.2.2 Models for threshold exceedances The simplest threshold exceedance model for a non-identically distributed process is to split the process into blocks, so that the data within each block is assumed to be IID. The GPD model for IID data is then applied to the data in each block. For example, we could split the Reading ozone data into seasons and analyse each one separately, using the threshold exceedances model. However, the most common approach takes the threshold exceedances approach and models the non-identical margins directly through the inclusion of covariates in the model parameters, in a similar manner to the covariate models for block maxima and \\(r\\)-largest observations. This approach was popularised by the paper of Davison and Smith (1990). The existing method The only difference between the existing threshold exceedance approaches for identically and non-identically distributed processes are that families of covariates are incorporated into the GPD and rate parameters in the non-stationary case. We shall denote the covariates associated with \\(\\{Y_t\\}\\) by \\(\\mathbf{X}_t\\). The modelling approach is then Pick a constant threshold, \\(u\\); Let \\(\\phi_u=\\Pr(Y_t&gt;u)\\), then model this exceedance probability by \\(\\phi_u=\\phi_u(\\mathbf{x}_t)\\); Model the exceedances of \\(u\\) by a GPD distribution with parameters \\(\\sigma_u(\\mathbf{x_t})\\) and \\(\\xi(\\mathbf{x}_t)\\). The rate and GPD scale parameters are forced to lie in the ranges \\(0 &lt; \\phi_u(\\mathbf{x}_t) &lt; 1\\) and \\(\\sigma_u(\\mathbf{x}_t)&gt;0\\). In the covariate models we ensure this by using, respectively, logit- and log-link functions. Note also that it is common to assume that the shape parameter \\(\\xi\\) is constant, due to the large amount of data needed to model this parameter well. However we continue to think of it as a function of the covariates, in order to work with the most general form of the model. Functions of the parameters \\(\\phi_u(\\mathbf{x}_t),\\sigma_u(\\mathbf{x_t}),\\xi(\\mathbf{x}_t)\\) are usually taken to be linear functions of the covariates: \\[\\begin{eqnarray*} \\mathrm{logit}~\\phi_u(\\mathbf{x}_t)=\\boldsymbol{\\phi}&#39;\\mathbf{x}_t,~\\log \\sigma_u(\\mathbf{x}_t)=\\boldsymbol{\\sigma}_u&#39;\\mathbf{x}_t,~\\mathrm{and}~\\xi(\\mathbf{x}_t)=\\boldsymbol{\\xi}&#39;\\mathbf{x}_t. \\end{eqnarray*}\\] However, other work has looked at fitting non-parametric (Hall and Tajvidi 2000) and additive (Chavez-Demoulin and Davison 2005) models instead. We shall stay within the linear framework. Inference We have chosen to work in the likelihood framework under which the non-identically distributed models can be easily fitted. The likelihood for the existing model is \\[\\begin{eqnarray*} \\small L(\\boldsymbol{\\phi}_u,\\boldsymbol{\\sigma}_u,\\boldsymbol{\\xi})=\\prod_{t=1}^n (1-\\phi_u(\\mathbf{x}_t))^{1-I_t} \\left(\\frac{\\phi_u(\\mathbf{x}_t)}{\\sigma_u(\\mathbf{x}_t)}\\left[1+\\xi(\\mathbf{x}_t)\\left(\\frac{y_t-u}{\\sigma_u(\\mathbf{x}_t)}\\right)\\right]_+^{-1/\\xi(\\mathbf{x}_t)-1}\\right) ^{I_t} \\end{eqnarray*}\\] where \\(I_t=I[Y_t&gt;u]\\) is the indicator function taking the value 1 if the observation is an exceedance and zero otherwise. The covariates can be selected using standard techniques from linear regression e.g.forward selection in which nested models are compared using the likelihood ratio test. Return levels In Section 3.2.6 we looked at estimating return levels for the GEV. The conditional \\(1/p\\) return level for the non-identically distributed GPD model satisfies \\(\\Pr(Y_t &gt; y_{t,p}|\\mathbf{X}_t=\\mathbf{x}_t) = p\\). If \\(y_{t,p}&gt;u\\) and \\(\\phi(\\mathbf{x}_t)&gt;p\\), this is given by \\[\\begin{eqnarray*} y_{t,p} = u + \\frac{\\sigma_u(\\mathbf{x}_t)}{\\xi(\\mathbf{x}_t)}\\left[\\left(\\frac{\\phi(\\mathbf{x}_t)}{p}\\right)^{\\xi(\\mathbf{x}_t)}-1\\right]. \\end{eqnarray*}\\] To obtain the marginal return level, i.e. \\(\\Pr(Y_t&gt;y_p)=p\\), we must integrate out the covariates. In the absence of information on their joint distribution we estimate this empirically so that \\(y_p\\) satisfies \\[\\begin{eqnarray*} p=\\frac{1}{n}\\sum_{t=1}^n \\phi_u(\\mathbf{x}_t)\\left[1+\\xi(\\mathbf{x}_t)\\frac{y_p-u}{\\sigma_u(\\mathbf{x}_t)}\\right]_+^{-1/\\xi(\\mathbf{x}_t)} . \\end{eqnarray*}\\] This can be solved for \\(y_p\\) using a numerical root-finder. 4.3 Pre-processing method An alternative method to model the extremes of a non-identically distributed process \\(\\{Y_t\\}\\) is to pre-process (or pre-whiten) the process to obtain a process that is much closer to the IID assumption than the original one. We then model the extremes of the pre-processed process. This method was developed by Eastoe and Tawn (2009). The advantages of this method include computational simplicity; better interpretability; greater efficiency; better theoretical justification. Pre-processing Aim: transform the process \\(\\{Y_t\\}\\) to have identical margins. One way to do this is to assume the non-identical margins are formed because of a trend in the mean and/or variance, as shown in the examples of section 4.1. We propose going one step further and allow the shape of the underlying marginal distributions of the process to have a trend also. We model this through a Box-Cox parameter \\(\\lambda(\\mathbf{x}_t)\\). Recall that \\(\\{Z_t\\}\\) is an IID process. This model is equivalent to assuming that \\[\\begin{equation} \\frac{Y_t^{\\lambda(\\mathbf{x}_t)}-1}{\\lambda(\\mathbf{x}_t)} = \\beta(\\mathbf{x}_t) Z_t + \\alpha(\\mathbf{x}_t). \\tag{4.3} \\end{equation}\\] Having obtained the series \\(\\{Z_t\\}\\) we model the tails using existing methods; Pick a constant threshold, \\(u_z\\); Let \\(\\phi_{z,u}=\\Pr(Z_t&gt;u)\\), then model this exceedance probability by \\(\\phi_{z,u}=\\phi_{z,u}(\\mathbf{x}_t)\\); Model the exceedances of \\(u_z\\) by a GPD distribution with parameters \\(\\sigma_{z,u}(\\mathbf{x_t})\\) and \\(\\xi_z(\\mathbf{x}_t)\\). In an ideal world, in which the marginal distribution is perfectly captured in the pre-processing parameters \\((\\alpha(\\mathbf{x}_t),\\beta(\\mathbf{x}_t),\\lambda(\\mathbf{x}_t))\\) we would not need parameters in the exceedance or GPD covariates. In reality, the pre-processed data will not be IID, perhaps due to missing covariates or because the covariate models are not sensitive enough to pick up subtle relationships, hence the need for covariates. Inference Under the likelihood framework this model is simple to fit. First the pre-processing parameters \\((\\alpha(\\mathbf{x}_t),\\beta(\\mathbf{x}_t),\\lambda(\\mathbf{x}_t))\\) are estimated. There is a range of ways to do this, for simplicity we assume the margins are normal. The process is then transformed using equation (4.3) and the extremes model fitted following the existing method. Conditional and marginal return levels are found in exactly the same manner as for the existing method, except that they must be back-transformed to the original scale using equation (4.3). Thus if \\(z_{p,t}\\) satisfies \\(\\Pr(Z_t&gt;z_{t,p})=p\\) the conditional return levels on the original scale are \\[\\begin{eqnarray*} y_{t,p}=[\\lambda(\\mathbf{x}_t)(\\beta(\\mathbf{x}_t)z_{t,p}+\\alpha(\\mathbf{x}_t))+1]^{1/\\lambda(\\mathbf{x}_t)}. \\end{eqnarray*}\\] Due to the two-stage nature of the inference procedure of the pre-processing method, confidence intervals for the return levels must be found by bootstrap methods, resampling from the pre-processed data \\(\\{Z_t\\}\\). Efficiency Figure 4.4 compares the efficiency of the existing and pre-processing models for estimating \\(\\alpha_1\\) in models \\(Y_t=Z_t+\\alpha(t)\\) where \\(\\alpha(t) = \\alpha_0 + \\alpha_1 t\\) (LHS) \\(\\alpha(t) = \\alpha_0 + \\alpha_1 \\sqrt 2\\cos(\\frac{2\\pi t}{N}+\\frac{\\pi}{4})\\) (RHS) In these plots, dashed lines show the expected efficiency loss since the existing method uses only threshold exceedances. Figure 4.4: Efficiency of pre-processing and existing methods for modelling threshold exceedances 4.4 Analysis of Reading ozone data We analyse the Reading ozone data using existing and pre-processing methods and compare the results. Following the previous discussion on the science behind the production of atmospheric ozone, we consider daily maxima of NO, NO\\(_2\\) and temperature, aggregate daily sunshine and a year-on-year trend as covariates. We also consider first-order interactions of these. The meteorological data has been obtained from a site situated less than 2km from the air quality monitoring site, courtesy of the UK Met. Office. We use maximum likelihood estimation throughout and forward selection to choose our covariates. Figure 4.5 shows the daily maxima of NO (\\(\\mu\\mathrm{gm}^{-3}\\)), NO\\(_2\\) (\\(\\mu\\mathrm{gm}^{-3}\\)) and temperature (\\(^{\\circ}\\)C) and daily aggregate hours of sunshine. Figure 4.5: Ozone covariates. Pre-processing parameters It is standard practise to assume that atmospheric data sets, such as the Reading ozone data, are (log)-normal. In a procedure similar to quasi-likelihood we assume the data to be normal in our estimation of the pre-processing parameters \\((\\alpha(\\mathbf{x}_t),\\beta(\\mathbf{x}_t),\\lambda(\\mathbf{x}_t))\\). The best fitting models are \\[\\begin{eqnarray*} \\lambda(\\mathbf{x}_t) &amp; = &amp; \\lambda_0 + \\lambda_1 \\mathrm{NO}_2 + \\lambda_2\\mathrm{NO} + \\lambda_3\\mathrm{Temp} + \\lambda_4 \\mathrm{Sun} + \\lambda_5 \\mathrm{Year} \\\\ &amp;&amp;- \\lambda_6\\mathrm{NO}_2 \\times \\mathrm{Temp} - \\lambda_7\\mathrm{NO}_2 \\times \\mathrm{Sun} - \\lambda_8 \\mathrm{Temp}\\times \\mathrm{Sun}\\\\ \\alpha(\\mathbf{x}_t) &amp; = &amp; \\alpha_0 + \\alpha_1 \\mathrm{NO}_2 - \\alpha_2\\mathrm{NO} + \\alpha_3\\mathrm{Sun} + \\alpha_4\\mathrm{NO}_2 \\times \\mathrm{NO}\\\\ &amp;&amp; -\\alpha_5\\mathrm{NO}_2 \\times \\mathrm{Sun} +\\alpha_6\\mathrm{NO} \\times \\mathrm{Sun}\\\\ \\log \\beta (\\mathbf{x}_t) &amp; = &amp; \\beta_0 + \\beta_1\\mathrm{NO} -\\beta_2\\mathrm{Year}\\\\. \\end{eqnarray*}\\] Thresholds Figure 4.6 shows the data with the 90% constant (existing method) and varying (pre-processing method) thresholds. Note how closely the varying threshold follows the data. Figure 4.6: Daily maximum ozone with pre-processing threshold shown. Exceedances parameter Figure 4.7 shows estimates of the exceedance parameter \\(\\phi_u(\\mathbf{x}_t)\\). The estimate is constant for the pre-processing method, but extremely variable for the existing method. Notice that the probability of an exceedance under the existing method is also very high (\\(&gt; 0.5\\)) in the summer months - are such data really extreme? Figure 4.7: Rate parameters for the pre-processing and existing methods for modelling threshold exceedances. GPD parameters MLE’s of the parameters in the best fitting model for the exceedances of the 90% threshold using the existing method are \\[\\begin{eqnarray*} \\mathrm{logit}~\\phi_{z,u}(\\mathbf{x}_t) &amp; = &amp; -5.53-0.0371\\mathrm{NO}+0.0127\\mathrm{NO}_2+0.137\\mathrm{Temp}\\\\ &amp; &amp; +0.273\\mathrm{Sun}+0.000209\\mathrm{NO}\\times\\mathrm{NO}_2;\\\\ \\log \\sigma_{z,u} &amp; = &amp; 1.18+0.0848\\mathrm{Temp}+0.0578\\mathrm{Sun}; \\\\ \\xi(\\mathbf{x}_t) &amp; = &amp; -0.337; \\end{eqnarray*}\\] and using the pre-processing method are \\[\\begin{eqnarray*} \\phi_{z,u}(\\mathbf{x}_t) &amp; = &amp; 0.100; \\\\ \\sigma_{z,u} &amp; = &amp; 0.670; \\\\ \\xi(\\mathbf{x}_t) &amp; = &amp; -0.192. \\end{eqnarray*}\\] In this case the pre-processing method results in a much simplified tail model which is therefore much easier to fit. Conditional return levels Point estimates (dots) of 99% return levels, with 95% bootstrapped confidence intervals (vertical lines) are shown in Figure 4.8. Crosses denote observed data. Figure 4.8: 99% return levels, with 95% confidence intervals, from both pre-processing and existing methods. Crosses show observed data. Finally, Figure 4.9 displays box plots of the 95% confidence interval widths for the two methods. The confidence intervals are generally much wider when we use the existing method than when we use the pre-processing method. Figure 4.9: Boxplots of bootstrapped 95% confidence intervals of the 99% return levels shown in previous figure. 4.5 Further reading This paper by Wadsworth, Tawn, and Jonathan (2010) also looked at Box-Cox transform in the context of extremes. Bibliography "],["stationary.html", "5 Extremes of Stationary Processes 5.1 Background 5.2 Unified Extremal Types Theorem for stationary sequences 5.3 The Extremal Index", " 5 Extremes of Stationary Processes Here, we will explore the impact of dependence between values in the series on the extreme values when observed at long-range and short-range. As previously we will focus on \\(M_n=\\max(X_1, \\ldots ,X_n)\\); exceedances of a high threshold. 5.1 Background A process \\(\\{X_t\\}\\) is said to be a stationary process if the joint distributions of \\[\\begin{eqnarray*} (X_{t_1}, \\ldots ,X_{t_k})\\mbox{ and } (X_{t_1+\\tau}, \\ldots ,X_{t_k+\\tau}) \\end{eqnarray*}\\] are the same for any \\(k\\), \\(t_1, \\ldots ,t_k\\) and \\(\\tau\\). Throughout we will assume that the univariate marginal distribution function is \\(F\\), i.e. \\[\\begin{eqnarray*} F(x)=\\Pr(X_t\\leq x) \\mbox{ for all }t. \\end{eqnarray*}\\] 5.1.1 Long-range asymptotic independence condition We suppose that there exist normalising sequences \\(a_{n}\\) and \\(b_{n}\\) such that \\[\\begin{eqnarray*} \\frac{M_{n} - b_{n}}{a_{n}} \\end{eqnarray*}\\] has a non-degenerate limit distribution. We want to characterise the limit behaviour of this variable. In the absence of any conditions that limit the amount of long-range dependence that can be present in the values of the series, any limit distribution can be obtained. For example let \\(X_t=X_1\\) for all \\(t\\), then for all \\(n\\) \\[\\begin{eqnarray*} \\Pr(M_n\\leq x)=\\Pr(X_1\\leq x)=F(x), \\end{eqnarray*}\\] so the class of limit distributions covers all distributions. Again, let’s see what we can learn from the CLT. A CLT for stationary processes exists and this also has to limit long-range dependence. The condition is termed strong mixing, it ensures that events separated sufficiently in time are effectively independent and so allows there to be enough independent variables for a CLT on these variables to apply. 5.1.2 Asymptotic independence of Maxima (AIM) Let \\(M_{i,j}=\\max(X_i, \\ldots, X_j)\\) and \\(u_{n} = a_{n} x + b_{n}\\) for \\(a_n, b_n\\) defined above and \\(x\\) any real number. The AIM\\((u_n)\\) condition is that there exists a sequence \\(q_{n}\\) of positive integers with \\(q_{n} = o(n)\\) such that for all \\(i\\) and \\(j\\) \\[\\begin{align} \\max |&amp;\\Pr (M_{1,i} \\leq u_{n} , M_{i+q_{n}, i+q_{n}+j } \\leq u_{n}) - \\nonumber\\\\ &amp;\\Pr (M_{1,i} \\leq u_{n}) \\Pr (M_{1,j} \\leq u_{n}) | \\rightarrow 0\\mbox{ as }n \\rightarrow \\infty. \\label{eq:AIM} \\end{align}\\] The condition ensures that separated groups of extreme points become increasingly close to being independent as their separation and level both increase at appropriate rates. There are many other similar conditions depending on the particular extremal property that is of interest, see Leadbetter, Lindgren, and Rootzén (1983) and O’Brien (1987). 5.2 Unified Extremal Types Theorem for stationary sequences Theorem Suppose that there exist normalising sequences \\(a_{n}\\) and \\(b_{n}\\) such that \\[\\begin{eqnarray*} \\Pr\\left(\\frac{M_{n} - b_{n}}{a_{n}}\\leq x\\right) \\rightarrow H(x) \\mbox{ as } n\\rightarrow \\infty, \\end{eqnarray*}\\] where \\(H\\) is a non-degenerate distribution, and that the AIM\\((a_n x+b_n)\\) condition holds, then \\(H\\) is of the same type as \\[\\begin{eqnarray*} \\exp\\left[-(1+\\xi x)_+^{-1/\\xi}\\right], \\end{eqnarray*}\\] i.e. a GEV limit distribution. Outline strategy of proof partition time into a series of alternating long and short blocks, \\(k\\) of each; let the numbers in long and short blocks be \\(r_n\\) and \\(q_n\\) respectively with \\(r_n\\rightarrow \\infty\\), \\(q_n\\rightarrow \\infty\\) and \\(q_n/r_n\\rightarrow 0;\\) in a neighbouring pair of long and short blocks, the maximum over the two blocks is likely to be in the long block; all maxima tend to fall into the long blocks; long block maxima are approximately independent as they are separated in time by the short blocks, so satisfy AIM\\((u_n)\\) condition; long block maxima satisfy the max-stability property, and hence are GEV. A few comments on this theorem: at a practical level this is all we need, the maximum of a stationary sequence that has some independence at long-range follows a GEV distribution; the result does not show how the dependence changes the behaviour of \\(M_n\\); We separate marginal and dependence features for the remainder of the chapter and subsequently fix the marginal distribution \\(F\\) of the \\(\\{X_t\\}\\) to be Fréchet. 5.2.1 Extremes of Fréchet marginal variables Let \\(\\{X_t\\}\\) be a stationary process which has Fréchet marginal distributions, i.e. \\[\\begin{eqnarray*} F(x)=\\exp(-1/x)\\mbox{ for }x&gt;0. \\end{eqnarray*}\\] Let \\(\\hat{M}_n\\) denote the maximum of \\(n\\) IID variables with marginal distribution \\(F\\) then \\[\\begin{eqnarray*} \\Pr\\left(\\frac{\\hat{M}_n}{n}\\leq x\\right)=\\{F(nx)\\}^n=\\exp(-1/x)=G(x), \\end{eqnarray*}\\] i.e. \\(\\hat{M}_n/n\\) has a Fréchet limit distribution for all \\(n\\) and we denote the limit distribution of the maximum of the IID variables by \\(G\\). Suppose that \\(M_n=\\max(X_1, \\ldots ,X_n)\\), that \\(\\{X_t\\}\\) satisfies the AIM\\((nx)\\) condition and that \\[\\begin{eqnarray*} \\Pr\\left(\\frac{M_n}{n}\\leq x\\right)\\rightarrow H(x), \\end{eqnarray*}\\] then we can assess the effect of dependence on \\(M_n\\) by looking at the difference between \\(H\\) and \\(G\\). 5.2.2 Motivating Example: Moving Maxima Let \\(\\{Y_t\\}\\) be IID with \\(F_Y(y)=\\exp[-1/(2y)]\\), i.e. Fréchet type marginals. Define, for all \\(t\\) \\[\\begin{eqnarray*} X_t=\\max(Y_t,Y_{t-1}) \\end{eqnarray*}\\] An example of this process is shown in Figure 5.1. Figure 5.1: Example of a moving maxima process. Marginal properties The marginal distribution of the \\(\\{X_t\\}\\) process is: \\[\\begin{eqnarray*} \\Pr(X_t\\leq x) &amp; = &amp; \\Pr(Y_t\\leq x,Y_{t-1}\\leq x)\\\\ &amp; = &amp; \\{F_Y(x)\\}^2\\\\ &amp; = &amp; \\exp[-2/(2x)]\\\\ &amp; = &amp; \\exp(-1/x), \\end{eqnarray*}\\] i.e. Fréchet distributed. Dependence properties For \\(\\tau&gt;1\\), \\((X_t,X_{t+\\tau})\\) is independent as the associated \\(Y\\) variables are all different for each of the \\(X\\)’s and hence the \\(X\\)’s are independent. For \\(\\tau=1\\), \\((X_t,X_{t+\\tau})\\) is dependent as the both \\(X_t\\) and \\(X_{t+1}\\) are functions of \\(Y_t\\); it is clear that long-range independence holds; extreme values occur as groups or clusters of independent values, with two values per cluster for all the largest values. Derivation of \\(H\\) \\[\\begin{eqnarray*} \\Pr\\left(\\frac{M_n}{n}\\leq x\\right) &amp; = &amp; \\Pr(X_1\\leq nx, \\ldots ,X_n \\leq nx)\\\\ &amp; = &amp; \\Pr(Y_0\\leq nx, Y_1\\leq nx, \\ldots ,Y_n\\leq nx)\\\\ &amp; = &amp; [F_Y(nx)]^{n+1}\\\\ &amp; = &amp; \\{\\exp[-1/(2nx)]\\}^{n+1}\\\\ &amp; = &amp; \\exp[-(n+1)/(2nx)]\\\\ &amp; \\rightarrow &amp; \\exp[-1/(2x)]\\mbox{ as }n\\rightarrow \\infty\\\\ &amp; = &amp; \\{G(x)\\}^{1/2}\\\\ &amp; = &amp; H(x). \\end{eqnarray*}\\] 5.3 The Extremal Index The previous example shows that short-range dependence in the extreme values of the process affects the limiting distribution of \\(M_n\\). Before expanding on this further, we first present a measure of short-range extremal dependence, termed the extremal index. Define the extremal index \\(\\theta\\), for variables with Fréchet marginals, by \\[\\begin{eqnarray*} \\theta &amp; = &amp; \\lim_{n\\rightarrow \\infty} \\Pr(M_{2,p_n}\\leq n\\,|\\, X_1&gt;n)\\\\ &amp; =&amp; \\lim_{n\\rightarrow \\infty} \\{1-\\Pr(M_{2,p_n}&gt; n\\,|\\, X_1&gt;n)\\} \\end{eqnarray*}\\] where \\(p_n=o(n)\\). The extremal index has the following features: \\(0\\le \\theta \\leq 1\\); larger values of \\(\\theta\\) correspond to weaker short-range extremal dependence; for an IID process \\(\\theta=1\\) as \\[\\begin{eqnarray*} \\theta &amp; = &amp; \\lim_{n\\rightarrow \\infty} \\{F(n)\\}^{p_n}\\\\ &amp; = &amp; \\lim_{n\\rightarrow \\infty} \\exp(-p_n/n)\\\\ &amp; \\rightarrow &amp; 1 \\mbox{ as }n\\rightarrow \\infty, \\end{eqnarray*}\\] (since \\(p_n/n\\rightarrow 0\\)); if a process is independent for all lags \\(\\tau\\ge m\\), for some finite \\(m\\) then \\(1/m\\le \\theta \\le 1\\). 5.3.1 Extremal index for Moving Maxima example From the realisation of the process it is clear that \\(\\theta=1/2\\). The theoretical derivation is \\[\\begin{eqnarray*} \\Pr(M_{2,p_n}\\leq n\\,|\\, X_1&gt;n) &amp; = &amp; \\Pr(Y_1\\leq n, Y_2\\leq n,\\ldots ,Y_{p_n}\\leq n\\,|\\, \\max(Y_0,Y_1)&gt;n)\\\\ &amp; \\approx &amp; \\Pr(Y_1\\leq n, Y_2\\leq n,\\ldots ,Y_{p_n}\\leq n\\,|\\, Y_0&gt;n)\\times\\frac{1}{2}\\\\ &amp; &amp; + \\Pr(Y_1\\leq n, Y_2\\leq n,\\ldots ,Y_{p_n}\\leq n\\,|\\, Y_1&gt;n)\\times\\frac{1}{2}\\\\ &amp; \\rightarrow &amp; 1\\times \\frac{1}{2}+0\\times \\frac{1}{2}\\\\ &amp; = &amp; \\frac{1}{2}=\\theta. \\end{eqnarray*}\\] 5.3.2 Implications of short-range extremal dependence Combining the above results and examples, the following result should appear to be quite natural. For variables with Fréchet marginal distributions, provided that \\(\\Pr(M_n/n\\leq x)\\rightarrow H(x)\\) as \\(n\\rightarrow \\infty\\) to a non-degenerate limit \\(H\\) then if AIM\\((nx)\\) condition holds; \\(\\theta\\) exists then \\[\\begin{eqnarray*} H(x)=\\{G(x)\\}^{\\theta}. \\end{eqnarray*}\\] The implications of this result are that, except for a really weak long-range dependence condition: for stationary processes the effect of dependence is through \\(\\theta\\) only; \\(\\theta\\) can be absorbed into the normalising constants, so dependence does not change the limiting type; the limit suggests \\[\\begin{eqnarray*} H(x)=\\lim_{n\\rightarrow \\infty}\\left[\\{F(nx)\\}^n\\right]^{\\theta} =\\lim_{n\\rightarrow \\infty}\\{F(nx)\\}^{n\\theta}, \\end{eqnarray*}\\] so \\(n\\theta\\) can be thought of as an effective number of independent variables. 5.3.3 Distribution of the Maximum Consider random variables \\(X_1, \\ldots ,X_n\\) with arbitrary distribution function \\(F\\) and which satisfy the AIM\\((u_n)\\) condition. As \\(n\\rightarrow \\infty\\) \\[\\begin{eqnarray*} \\Pr\\left(\\frac{M_n-b_n}{a_n}\\leq y\\right) \\rightarrow \\exp\\left[-\\theta \\left\\{1+\\xi\\left(\\frac{y-\\mu}{\\sigma}\\right)\\right\\}_+^{-1/\\xi}\\right], \\end{eqnarray*}\\] where if the variables are IID \\(\\theta=1\\). 5.3.4 Threshold exceedances We assume that \\(\\{X_t\\}\\) is a stationary process with arbitrary distribution function \\(F\\); the required norming constants for an IID process with marginal \\(F\\) are \\(a_n\\) and \\(b_n\\) with limit distribution \\(G\\), a GEV\\((0,1,\\xi)\\) distribution; a long-range asymptotic independence condition (similar to the AIM\\((a_nx+b_n)\\) condition) holds. Consider the point processes \\[\\begin{eqnarray*} P_n=\\left\\{\\left(\\frac{i}{n+1},\\frac{X_i-b_n}{a_n}\\right);i=1, \\ldots ,n\\right\\} \\end{eqnarray*}\\] on \\([0,1]\\times [0,\\infty)\\). Figure 5.2 shows a point process \\(P_n\\) for \\(n=10,100,1000,10000\\) respectively with \\[ X_t = \\max(\\alpha_0Y_t,~\\alpha_1Y_{t-1},~\\alpha_2Y_{t-2}), \\] where \\(\\{Y_t\\}\\) is an IID process as before, and \\(\\alpha_0=\\frac{1}{3}\\), \\(\\alpha_1=\\frac{1}{2}\\), and \\(\\alpha_2=\\frac{1}{6}\\). Equivalently, we say \\(X_t\\sim\\) Moving Max \\(\\left(\\alpha_0=\\frac{1}{3},\\alpha_1=\\frac{1}{2},\\alpha_2=\\frac{1}{6}\\right)\\). Figure 5.2: Point process \\(P_n\\) representation of the moving maxima process for \\(n=10,100,1000,10000\\). 5.3.5 Cluster maxima point process intensity For data over a high threshold \\(u\\) we absorb the location and scale normalisation into the parameters to give the intensity of the non-homogeneous Poisson Process for the cluster maxima as \\[\\begin{eqnarray} \\lambda(t,y)=\\theta \\sigma^{-1}[1+\\xi (y-\\mu)/\\sigma]_+^{-1-1/\\xi} \\label{eqn:clust.max.intensity} \\end{eqnarray}\\] Implications of the limit process: clusters Fixing a threshold level \\(u&gt;b_l\\) we have that: clusters are those set of points exceeding \\(u\\) which occur at the same normalised time. In the original series these are large points which occur within a short time period of one another. the expected number of exceedances of \\(u\\) per cluster (for which the cluster maxima exceeds \\(u\\)) is \\(\\theta^{-1}\\), irrespective of the level \\(u\\); relative to independent series, when \\(\\theta&lt;1\\) there are fewer clusters (by a factor \\(\\theta\\)) and more exceedances per cluster (by a factor \\(\\theta^{-1}\\)); values in one cluster are independent of values in another cluster; values within a cluster are dependent. Implications of the limit process: exceedances Fixing a threshold level \\(u&gt;b_l\\) we have that: the rate of cluster maxima exceeding the threshold \\(u\\) is \\[\\begin{eqnarray*} \\theta [1+\\xi(u-\\mu)/\\sigma]_+^{-1/\\xi}. \\end{eqnarray*}\\] the rate of arbitrary values exceeding the threshold \\(u\\) is \\[\\begin{eqnarray*} [1+\\xi(u-\\mu)/\\sigma]_+^{-1/\\xi}. \\end{eqnarray*}\\] the cluster maxima that exceed the threshold are independent, and follow a GPD\\((\\sigma_u,\\xi)\\), with shape parameter \\(\\xi\\); GPD\\((\\sigma_u,\\xi)\\) model for cluster maxima excesses over \\(u\\), where \\[\\begin{eqnarray*} \\sigma_u=\\sigma+\\xi(u-\\mu). \\end{eqnarray*}\\] arbitrary (in time) excesses over \\(u\\) also follow a GPD\\((\\sigma_u,\\xi)\\), i.e.  with the same parameters as the cluster maxima; Bibliography "],["inference-dependent.html", "6 Statistical Inference for Extremes of Stationary Processes 6.1 Identifying independent clusters 6.2 Modelling cluster maxima and all exceedances", " 6 Statistical Inference for Extremes of Stationary Processes We now consider the additional practical difficulties encountered due to dependence in analysing the extreme values of a stationary process. In a practical data analysis we have to: select a high threshold; identify clusters of extreme values which are independent from one another; estimate the extremal index; When clusters are defined we extract the cluster maxima and fit the GPD distribution to the cluster maxima over the threshold; estimate characteristics of the cluster; assess sensitivity of cluster characteristics to threshold selection. 6.1 Identifying independent clusters There are a number of ways of identifying independent clusters of extreme values. Here we will focus on the runs method, proposed by Smith and Weissman (1994), but blocks and intervals methods also exist (Ferro and Segers 2003), and Ledford and Tawn (2003) identified conditions that could be tested for independence between clusters. 6.1.1 The runs method: declustering For a selected high threshold \\(u\\) we conclude that if consecutive exceedances of the threshold \\(u\\) are separated by a set of \\(m\\) consecutive observations below the threshold \\(u\\) then the exceedances belong to separate clusters. Similarly, exceedances separated by less than \\(m\\) consecutive non-exceedances are deemed to be in the same cluster. The choice of \\(u\\) and \\(m\\) is critical, \\(m\\) needs to be the time lag when the process is independent in the extremes. The runs method: extremal index estimation This algorithm motivates the runs estimator of the extremal index \\(\\theta\\). Use the empirical form the extremal index, \\[\\begin{eqnarray*} \\hat{\\theta}(u,m) &amp; = &amp; \\hat{\\Pr}(M_{2,m+1}\\leq u\\,|\\, X_1&gt;u), \\end{eqnarray*}\\] using the empirical probability. \\(\\hat{\\theta}(u,m)\\) is identical to \\(1/\\mbox{mean cluster size}\\) with clusters defined by the runs method. Example: daily rainfall data We illustrate the use of these cluster identification and extremal index estimation using a time series of daily rainfall accumulations at a location in south-west England, recorded during 1914-1962. This data set is shown in Figure 6.1. Note that the series is assumed to be stationary over this observation period. Figure 6.1: Daily rainfall. Figure 6.2 shows a plot of the extremal index of the rainfall data, for \\(m=1\\) and \\(m=2\\). From this we can see that \\(\\hat{\\theta}(u,m)\\) are threshold dependent increasing \\(m\\) does not materially change our estimate \\(\\hat{\\theta}(u,m)\\) \\(m=1\\) is appropriate for cluster identification the value of \\(\\theta(u,m)\\approx 1\\) indicates weak short-range dependence with a limiting cluster size of 1 for higher thresholds. from experience we know rainfall episodes last a number of days, but this analysis tells us that extreme daily rainfall events tend to be isolated. Figure 6.2: Extremal index estimates using the runs estimator with \\(m=1\\) or \\(m=2\\), for the rainfall data shown above. Example: financial time series data We now turn to estimating clustering for the financial time series data. We focus on the squared returns from the FTSE 100 share index; these are shown in Figure 6.3. We make the initial assumption that the series is stationary over this observation period. (Note: this is slightly different to the observation period shown in Figure 1.4.) Figure 6.3: Squared returns for the FTSE 100 share index. 6.1.1.1 Extremal index for the squared FTSE returns The extremal index for the squared returns shown in Figure 6.3 is shown in Figure 6.4. From this plot we can see that \\(\\hat{\\theta}(u,m)\\) is stable for \\(u\\geq 0.005\\) corresponding to the 0.95 quantile of the squared returns; the value of \\(\\hat{\\theta}(u,m)\\approx0.35\\) suggests that the mean cluster size is around 2.9; \\(m=10\\) seems fine for cluster identification. Figure 6.4: Extremal Index of the squared returnes for the FTSE 100 data, estimated using the runs method with \\(m=10\\). We now investigate the source of serial dependence being the changing volatility of the process. We first standardise the returns series by subtracting the local mean dividing by the local standard deviation We work with the squared standardised series as shown in Figure 6.5. Local means and standard deviations were calculated using the \\(h=20\\) observations centred on the value to be standardised. There is little sensitivity to this choice of \\(m\\) for values of \\(h\\) between 2 and 50. Figure 6.5: Standardised squared returns for the FTSE 100 series. Figure 6.6: Extremal index for the standardised squared returns of the FTSE 100 series. Estimates made using the runs method with \\(m=1\\) and \\(m=2\\). The extremal index for the squared standardised returns is shown in Figure 6.6. From this plot, estimates of \\(\\theta(u,m)\\) are threshold dependent; increasing \\(m\\) does not materially change our conclusions, \\(m=1\\) is fine for cluster identification; the limiting value of \\(\\theta=1\\) indicates weak short-range independence with a limiting cluster size of 1; by undertaking the more sophisticated analysis and standardising the series we appear to have accounted for all the short-range dependence in the series. 6.2 Modelling cluster maxima and all exceedances For a specific threshold \\(u\\), we can model the cluster maxima fit the GPD\\((\\sigma_u,\\xi)\\) model for cluster maxima excesses over \\(u\\). standard errors can be obtained by usual methods as cluster maxima are independent. model all exceedances fit the GPD\\((\\sigma_u,\\xi)\\) model for the excess of all exceedances over \\(u\\). the data are dependent so block bootstrap methods are needed for standard error evaluation. 6.2.1 Point process intensity estimation To estimate \\(\\mu\\) and \\(\\sigma\\) in the point process intensity we need information in addition to that given by our estimate of \\(\\sigma_u\\) and \\(\\xi\\). This information is given by the expected number of cluster maxima exceeding \\(u\\). Equating the point process and sample values gives \\[\\begin{eqnarray*} \\theta [1+\\xi(u-\\mu)/\\sigma]_+^{-1/\\xi}=n_{cl,u}. \\end{eqnarray*}\\] It follows that \\[\\begin{eqnarray*} \\hat{\\sigma}=\\hat\\sigma_u(n_{exc,u})^{\\hat{\\xi}}\\mbox{ and } \\hat{\\mu}=u+(\\hat{\\sigma}-\\hat{\\sigma}_u)/\\hat{\\xi}. \\end{eqnarray*}\\] Note that \\(\\theta\\) is estimated by \\(\\hat{\\theta}_u =n_{cl,u}/n_{exc,u}\\), i.e. the number of clusters above \\(u\\) divided by the number of exceedances of \\(u\\). Example: Squared FTSE returns For the squared FTSE returns using a declustering parameter of \\(m=10\\) and a threshold of \\(u=0.004\\) corresponding to the 0.95 quantile of the squared returns gives \\(n_{cl,u}=134\\) independent clusters there are \\(n_{exc,u}=429\\) exceedances of \\(u\\) we estimate the extremal index as \\(\\hat\\theta=0.31\\) fitting a GPD to cluster maxima gives \\[\\begin{eqnarray*} \\hat\\sigma_u = 0.000232 (0.0000679) ~~~\\hat\\xi = 0.653 (0.152) \\end{eqnarray*}\\] combining the above estimates gives \\[\\begin{eqnarray*} \\hat{\\sigma}=\\hat\\sigma_u(n_{exc,u})^{\\hat{\\xi}} = 0.012\\mbox{ and } \\hat{\\mu}=u+(\\hat{\\sigma}-\\hat\\sigma_u)/\\hat{\\xi} = 0.0223. \\end{eqnarray*}\\] Bibliography "],["finance.html", "7 Extremes in Finance 7.1 The Hill estimator for heavy-tailed data 7.2 Modelling dependence 7.3 Value at Risk, Expected Shortfall, &amp; Volatility 7.4 Volatility models 7.5 Further reading", " 7 Extremes in Finance In this chapter, we look at methods in extreme value theory that are suitable for applications to financial data. However, it should not be mistaken that they are not applicable to other kinds of data e.g. environment data. 7.1 The Hill estimator for heavy-tailed data Financial data almost always have heavy tails \\((\\xi&gt;0)\\). Sometimes a simpler model is assumed for the survivor function, which is a good approximation to the GPD for large values. We assume \\[\\begin{eqnarray} \\Pr(X&gt;x) \\approx \\frac{c}{x^{\\alpha}},~~~c&gt;0, \\alpha&gt;0, x&gt;u, \\tag{7.1} \\end{eqnarray}\\] for \\(u\\) a high threshold. Thus we have \\[\\begin{eqnarray} \\Pr(X&gt;x|X&gt;u) \\approx (u/x)^\\alpha. \\tag{7.2} \\end{eqnarray}\\] This model is very similar to the GPD for \\(\\xi&gt;0\\) and large \\(x\\). We begin with the GPD expression for \\(\\Pr(X&gt;x|X&gt;u)\\): \\[\\begin{align*} \\left[1+\\xi \\left(\\frac{x-u}{\\sigma_u}\\right)\\right]_+^{-1/\\xi} &amp; = \\left(\\frac{\\sigma_u}{\\xi}\\right)^{1/\\xi} x^{-1/\\xi} \\left[1+\\frac{\\sigma_u-\\xi u}{\\xi x}\\right]^{-1/\\xi}\\\\ &amp; = \\left(\\frac{\\sigma_u}{\\xi}\\right)^{1/\\xi} x^{-1/\\xi}[1+O(x^{-1})],~~x\\to\\infty \\end{align*}\\] Now \\(\\sigma_u = \\sigma+\\xi(u-\\mu)\\), so \\(\\sigma_u-\\xi u\\) does not depend on \\(u\\) and furthermore \\[\\begin{eqnarray*} \\left(\\frac{\\sigma_u}{\\xi}\\right)^{1/\\xi} = u^{1/\\xi}\\left(1+\\frac{\\sigma/\\xi-\\mu}{u}\\right)^{1/\\xi} \\approx u^{1/\\xi} \\end{eqnarray*}\\] for large \\(u\\). Therefore \\(\\Pr(X&gt;x|X&gt;u) \\approx (u/x)^{1/\\xi}\\), for large \\(x\\) and large \\(u\\), which is the expression (7.2) for \\(\\alpha=1/\\xi\\). 7.1.1 The Hill estimator of \\(\\alpha\\) The advantage of equation (7.1) is it lends itself to a simple, closed form estimator for \\(\\alpha\\) (equivalently \\(\\xi\\)). Although numerical methods now make maximisation of GPD likelihoods very simple, the Hill estimator is still widely used by many who work with heavy-tailed data. There is more than one way to derive the estimator: we shall stick with a likelihood motivation. According to (7.1), the density for an observation, given that it exceeds \\(u\\), is \\[\\begin{eqnarray*} f(x|X&gt;u) = \\alpha u^{\\alpha} x^{-\\alpha-1}. \\end{eqnarray*}\\] For a total of \\(n_u\\) threshold exceedances, this gives the MLE of \\(\\alpha\\) as \\[\\begin{eqnarray*} \\hat{\\alpha} = \\left[\\frac{1}{n_u} \\sum_{i=1}^{n_u} (\\log x_i - \\log u)\\right]^{-1}. \\end{eqnarray*}\\] Often the threshold \\(u\\) is actually taken to be the \\(k\\)-th order statistic, \\(X_{(k)}\\). The choice of \\(k\\) is similar to the choice of \\(u\\), in the sense of the bias-variance trade-off involved. 7.1.2 Estimation of \\(c\\) Estimation of \\(c\\) can be performed by using the empirical proportion of exceedances to estimate the theoretical proportion: \\[\\begin{eqnarray*} \\Pr(X&gt;u) = \\frac{c}{u^{\\alpha}} =\\frac{n_u}{n} \\end{eqnarray*}\\] so \\(\\hat{c} = \\frac{n_u}{n} u^{\\hat{\\alpha}}\\). Plugging in the estimated \\(\\hat{\\alpha}\\), \\(\\hat{c}\\) into equation (7.1) provides a simple estimate of high quantiles. \\[\\begin{eqnarray*} x_p = \\left(\\frac{p}{\\hat{c}}\\right)^{-1/\\hat{\\alpha}}. \\end{eqnarray*}\\] 7.2 Modelling dependence Suppose we are interested in the risk associated to more than one stock. Tho assess this we need to model the dependence between the stocks at extreme levels. The figure shows the negative returns of BMW plotted against the negative returns of Siemens over the same period. There is a positive relationship, and we need to take this into account in calculating the risk of a portfolio containing both of these stocks. 7.2.1 Correlation The first measure of dependence people usually think of is correlation. \\[ \\begin{aligned} \\mbox{Corr}(X,Y) = \\frac{E(XY)-E(X)E(Y)}{\\sqrt{\\mbox{Var}(X)}\\sqrt{\\mbox{Var}(Y)}} \\end{aligned} \\] This measure: Requires existence of first and second moments Is not invariant to marginal choice Only fully characterises the dependence if \\((X,Y)\\) have a bivariate Gaussian distribution. 7.2.2 Alternative dependence measures Fitting a bivariate Gaussian distribution to the data with the estimated correlation coefficient is as arbitrary as fitting a Gaussian model to the margins and using the estimated tail for the extremes. As in the univariate case, we anticipate that dependence at extreme levels is different to that at “average” levels. We don’t want to contaminate our estimate of extremal dependence with average dependence. 7.2.2.1 Motivating examples Let us assume that our variables have standard Fréchet margins, so \\(\\Pr(X\\leq x) = e^{-1/x}\\). As \\(x\\to\\infty\\), \\[\\begin{eqnarray*} \\Pr(X&gt;x) = x^{-1}[1+O(x^{-1})]. \\end{eqnarray*}\\] Independence If \\((X,Y)\\) are independent then their joint survivor function evaluated on the diagonal can be expressed as \\[\\begin{eqnarray*} \\Pr(X&gt;x,Y&gt;x) = x^{-2}[1+O(x^{-1})]~~x\\to\\infty. \\end{eqnarray*}\\] Bivariate extreme value distribution If \\(\\Pr(X\\leq x, Y\\leq y) = \\exp\\{-V(x,y)\\}\\) with \\(V:(0,\\infty)^2\\to(0,\\infty)\\) homogeneous of order 1 we say \\((X,Y)\\) have a bivariate extreme value distribution. Then \\[\\begin{eqnarray*} \\Pr(X&gt;x,Y&gt;x) = x^{-1}[2-V(1,1)+O(x^{-1})],~~x\\to\\infty. \\end{eqnarray*}\\] Bivariate normal distribution If \\((\\Phi^{-1}(\\exp\\{-1/X\\}), \\Phi^{-1}(\\exp\\{-1/Y\\})) \\sim\\) N\\(_2(\\rho)\\) then \\[\\begin{eqnarray*} \\Pr(X&gt;x,Y&gt;x) = x^{-2/(1+\\rho)}[C_{\\rho} (\\log(x))^{\\rho/(1+\\rho)}+o(1)],~~x\\to\\infty. \\end{eqnarray*}\\] 7.2.3 The coefficient of tail dependence Let us assume that our variables have standard Fréchet margins (achieved by a transformation). Then a very general assumption is \\[\\begin{eqnarray} \\Pr(X&gt;x, Y&gt;x) = L(x)x^{-1/\\eta},~~~x\\to \\infty, \\tag{7.3} \\end{eqnarray}\\] where \\(L:(0,\\infty)\\to (0,\\infty)\\) is termed a slowly varying function3 \\(\\eta \\in(0,1]\\) is the coefficient of tail dependence Values of \\(\\eta\\) close to 0 indicate weak (negative) extremal dependence, \\(\\eta=1/2\\) corresponds to near independence, and \\(\\eta\\) near 1 corresponds to strong extremal dependence. Let us suppose that \\(L(x) \\approx C\\), since slowly varying functions do not change much over a large range of \\(x\\). We can rewrite equation (7.3) as \\[\\begin{eqnarray*} {\\Pr(\\min(X,Y)&gt;x) \\approx Cx^{-1/\\eta}}. \\end{eqnarray*}\\] Note the similarity with equation (7.1). We can use the Hill estimator to estimate \\(\\eta\\). For an interpretation of \\(\\eta\\), consider the conditional probability \\[\\begin{eqnarray*} \\Pr(X&gt;x | Y&gt;x) \\approx Cx^{1-1/\\eta}. \\end{eqnarray*}\\] If \\(\\eta=1\\) this probability is constant irrespective of the value of \\(x\\) (as long as \\(x\\) is large). Otherwise it decreases with \\(x\\), and this decrease is more rapid more smaller \\(\\eta\\). 7.2.4 Example: dependence in negative returns The first stage in the estimation is to transform the margins to standard Fréchet. We need the Probability Integral Transform (Section 3.2.5). If \\(X \\sim F\\), then \\(-1/\\log F(X) \\sim\\) Fréchet. Thus we need an estimate of \\(F\\). We could either Use the empirical distribution function (EDF) throughout Use the EDF up to a threshold \\(u\\), and the GPD above the threshold Option 1 means setting: \\[\\begin{eqnarray*} \\tilde{F}(X_i) = \\mbox{rank}(X_i)/(n+1). \\end{eqnarray*}\\] For Option 2, above a threshold \\(u\\), we set \\[\\begin{eqnarray*} \\hat{F}(X_i) = 1-\\hat{\\phi}_u[1+\\hat{\\xi}(X_i-u)/\\hat{\\sigma}_u]^{-1/\\hat{\\xi}}_+. \\end{eqnarray*}\\] Set \\(Z_i = \\min(X_i,Y_i)\\) \\[\\begin{eqnarray*} \\hat{\\eta} = \\frac{1}{n_u} \\sum_{i=1}^{n_u} (\\log z_i - \\log u) \\end{eqnarray*}\\] For \\(u\\) equal to the 0.95 quantile of the \\(Z_i\\) we have \\(\\hat{\\eta} = 0.916\\) (standard error 0.055), suggesting fairly strong extremal dependence. We can now estimate the level of the loss that is exceeded by both variables with small probability \\(p\\). \\[\\begin{eqnarray*} \\Pr(\\min(X,Y)&gt;z_p) \\approx \\hat{C} z_p^{-1/\\hat{\\eta}} = p, \\end{eqnarray*}\\] using the estimate of \\(C\\) as in Section 7.1.2. To translate the estimated \\(z_p\\) back to the returns scale we need to use the inverse of the earlier transformation to the Fréchet scale. This, amongst other reasons, suggests use of Option 2 is preferable for the transformation. 7.2.5 The extremal index and the coefficient of tail dependence The coefficient of tail dependence \\(\\eta\\) describes the dependence in the extreme values of two random variables. This is different to what we have seen in Chapter 5.3, where the extremal index \\(\\theta\\) describes the short-range temporal/serial dependence in the extreme values of stationary processes. Their difference is analogous to that between the Pearson correlation of two variables (Chapter 7.2.1) &amp; the autocorrelation function of a time-series. It is possible to investigate both kinds of dependence for the same set of variables. For example, we consider the variables \\(\\{X_t\\}\\) in a stationary (but dependent) process, where we can calculate the extremal index as usual. We can also compute the coefficient of tail dependence by considering \\(\\{X_t\\}\\) and \\(\\{X_{t-1}\\}\\) as the two variables of interest. 7.2.6 Multivariate extremes and further reading The coefficient of tail dependence can be extended to a multivariate setting and computed for all possible pairs of random variables. As multivariate extremes are beyond this module’s scope, we will include some references here. The original proponents of \\(\\eta\\) are Ledford and Tawn (1996) and Ledford and Tawn (1998), while Heffernan (2000) provided a comprehensive directory of \\(\\eta\\) for different bivariate distributions. Other related works include, in chronological order, Sibuya (1960), Mardia (1964), Tiago de Oliveira (1962/63), Pickands (1981), S. G. Coles and Tawn (1991), Ledford and Tawn (1997), and S. G. Coles, Heffernan, and Tawn (1999). A general reference on multivariate models and dependence concepts is Joe (1997). Finally, Heffernan and Tawn (2004) proposed a different yet influential conditional approach to multivariate extremes. 7.3 Value at Risk, Expected Shortfall, &amp; Volatility You may also have learned about Value at Risk (VaR), which is indeed equivalent to high quantiles and return levels we have seen in Section 2.3.3. In this section, we will look deeper into VaR &amp; a related quantity called Expected Shortfall (also known as Conditional VaR), and the effect of volatility in the calculations of VaR. 7.3.1 Fitting GEV Often in financial contexts, one is more concerned with the loss in a particular day, rather than the maximum losses over a particular time period (e.g. three months). How can we use the models we have learned to help with this estimation? Recall that, for an IID sequence \\(\\{X_t\\}\\) assumed to come from the distribution \\(F\\), \\[ \\Pr(M_n\\leq y)=F(y)^n\\approx G\\left(\\frac{y-b_n}{a_n}\\right), \\] where \\(M_n\\) is the block maxima of size \\(n\\) (e.g. maxima of \\(n\\) daily negative returns), and our estimated GEV distribution, \\(G\\left(\\frac{y-b_n}{a_n}\\right)=\\tilde{G}(y)\\) is approximately \\(F(y)^n\\). One way to translate our return levels to quantiles of the original distribution is to solve \\[ F(z_p)=\\tilde{G}^{1/n}(z_p)=p \\] giving \\[ z_p=\\mu-\\frac{\\sigma}{\\xi}\\left[1-\\left(-n\\log p\\right)^{-\\xi}\\right]. \\] Compare this to Equation (2.6). When referring to daily negative returns, this quantity is called the \\(100p\\)% VaR, which is the value that will be exceeded with probability \\(1-p\\). That is, for a series of negative daily returns, \\(X_t\\), \\[ \\Pr(X_t\\leq\\mbox{VaR}(p))=p \\qquad \\Leftrightarrow \\qquad \\Pr(X_t&gt;\\mbox{VaR}(p))=1-p. \\] Note that, the definition of VaR\\((p)\\) varies slightly in the literature, as, for example, the 99% VaR here may be called the 1% VaR elsewhere. Nevertheless, there usually is no ambiguity as investors are most interested in large potential losses. BMW example We continue with the daily negative returns seen above, but focus on one of the two stocks, namely BMW. Figure 7.1: Daily negative returns of BMW. The BMW data were maxima of \\(n=66\\) consecutive negative returns (which is approximately three months of trading days). There were \\(g=93\\) such maxima, so the maxima relate to a total of \\(T=ng=6138\\) daily negative returns. The \\(95\\)% VaR (that is, the negative return value that will be exceeded with probability \\(5\\)% on a given day) is \\[\\begin{eqnarray*} \\hat\\mu -\\frac{\\hat\\sigma}{\\hat\\xi}\\left[1-\\{-66\\log(0.95)\\}^{-\\hat\\xi}\\right] = 0.0153 \\end{eqnarray*}\\] The \\(99\\)% VaR is \\[\\begin{eqnarray*} \\hat\\mu -\\frac{\\hat\\sigma}{\\hat\\xi}\\left[1-\\{-66\\log(0.99)\\}^{-\\hat\\xi}\\right] = 0.0333 \\end{eqnarray*}\\] To obtain estimates of uncertainty, we could again use the delta method or profile likelihood. 7.3.2 Fitting GPD &amp; undoing the conditioning Suppose we fit the GPD to a series of negative returns. How do we calculate the VaR? We know \\[\\begin{eqnarray*} \\Pr(X&gt; x | X&gt;u) = \\left[1+\\xi\\left(\\frac{x-u}{\\sigma_u}\\right)\\right]^{-1/\\xi}_+. \\end{eqnarray*}\\] We need to undo the conditioning to calculate the quantile. The GPD provides a model for excesses conditional upon being an excess. We undo this conditioning by modelling the number of exceedances, \\(N_u\\), assuming either \\(N_u \\sim\\) Poisson\\((\\gamma_u), ~~~ \\gamma_u = E(N_u)\\) \\(N_u \\sim\\) Binomial\\((n,\\phi_u), ~~~ \\phi_u=\\Pr(X&gt;u)\\) The first option is equivalent to the Poisson process model of Section 3.1.4, with a different parameterisation. The second is almost equivalent for large sample sizes. The maximum likelihood estimate for \\({\\phi_u}\\) is \\({n_u/n}\\), with \\({N_u=n_u}\\) the observed number of exceedances of \\(u\\), from a total of \\(n\\) data points. VaR\\((p)\\) can thus be found by solving \\[\\begin{eqnarray*} 1-\\left[1+\\hat\\xi\\left(\\frac{\\mbox{VaR}(p)-u}{\\hat\\sigma_u}\\right)\\right]^{-1/\\hat\\xi} \\hat\\phi_u = p \\end{eqnarray*}\\] to give \\[\\begin{eqnarray*} \\mbox{VaR}(p) = u +\\frac{\\hat\\sigma_u}{\\hat\\xi}\\left[\\left(\\frac{\\hat\\phi_u}{1-p}\\right)^{\\hat\\xi}-1\\right]. \\end{eqnarray*}\\] BMW example We continue with the BMW example. We choose \\(u = 0.03\\), which means there are \\(136\\) exceedances (quite comparable to the number of block maxima). Using the parameter estimates, we can calculate the \\(95\\)% VaR: \\[\\begin{eqnarray*} u + \\frac{\\hat\\sigma_u}{\\hat\\xi}\\left[\\left(\\frac{\\hat\\phi_u}{1-0.95}\\right)^{\\hat\\xi}-1\\right] = 0.0203 \\end{eqnarray*}\\] The \\(99\\)% VaR is \\[\\begin{eqnarray*} u + \\frac{\\hat\\sigma_u}{\\hat\\xi}\\left[\\left(\\frac{\\hat\\phi_u}{1-0.99}\\right)^{\\hat\\xi}-1\\right] = 0.0406 \\end{eqnarray*}\\] We can see the estimated VaRs are similar but slightly larger than those by using the GEV for block maxima. 7.3.3 Expected Shortfall Expected Shortfall, \\({\\mbox{ES}(p)}\\), also known as conditional VaR, is the name given to the expected negative return, given that the VaR threshold has been exceeded. If \\({X}\\) denotes a negative return then \\[\\begin{eqnarray*} \\mbox{ES}(p) = E[X|X&gt;\\mbox{VaR}(p)]. \\end{eqnarray*}\\] Assuming \\(\\mbox{VaR}(p)\\) to be a high threshold, and we have fitted a GPD above a threshold \\({u&lt;\\mbox{VaR}(p)}\\), this is calculated using the expression in Section 3.2.2. \\[\\begin{align*} \\mbox{ES}(p) &amp;= \\mbox{VaR}(p) + \\frac{\\sigma_v}{1-\\xi},\\qquad\\qquad\\qquad(\\mbox{where}~v=\\mbox{VaR}(p))\\\\ &amp;= \\mbox{VaR}(p) + \\frac{\\sigma_u + \\xi(\\mbox{VaR}(p) - u)}{1-\\xi}. \\end{align*}\\] 7.3.4 Adjustment to VaR for dependent data Thus far we have ignored dependence in our VaR calculations. We have seen two methods for calculating VaR: Fitting the GEV to “subperiod maxima” (e.g. three-monthly maxima) and solving \\(\\tilde{G}^{1/n}(z_p) \\approx F(z_p) = p\\) Fitting the GPD to all exceedances and solving \\(P(X&gt;z_p|X&gt;u)\\phi_u \\approx 1-F(z_p) = 1-p\\) with \\(z_p = \\mbox{VaR}(p)\\). How do we calculate the VaR when it is evident that there is serial dependence in the data? Adjusting for subperiod maxima As seen in Section 5.3.2 our estimated GEV for a stationary process is \\(\\tilde{H}(x)\\approx F(x)^{n\\theta}\\). Thus we need to solve \\({\\tilde{H}^{1/(n\\theta)}(z_p) = p}\\): \\[\\begin{eqnarray*} \\hat{z_p}=\\hat\\mu -\\frac{\\hat\\sigma}{\\hat\\xi}\\left[1-\\{-n\\theta\\log p\\}^{-\\hat\\xi}\\right] \\end{eqnarray*}\\] with \\(z_p = \\mbox{VaR}(p)\\). If \\(\\theta&lt;1\\) this estimate is larger than when \\(\\theta=1\\). Ignoring dependence thus means we underestimate the VaR. Note that we will still need all data to estimate the extremal index (via the Runs Method). Adjusting for all exceedances The original estimate is still valid in this case. However, if we use all exceedances to estimate parameters, then an adjustment to the standard errors would be required (cf. Section 6.2). BMW example Going back to previous results, the estimates of the GPD parameters for the entire series taking a threshold of \\(u=0.03\\) are \\(\\hat{\\sigma}_u=0.0126~(0.00157)\\) and \\(\\hat{\\xi} = 0.143~(0.0947)\\). The quoted standard errors are too small as the exceedances are not independent. The estimates of the GPD parameters using clustering maxima for clusters defind by the runs method taking \\(u=0.03\\) and \\(m = 10\\) are \\(\\hat{\\sigma}_u=0.0126~(0.00189)\\) and \\(\\hat{\\xi} = 0.179~(0.116)\\). These standard errors are more reliable. The \\(99\\)% VaR for the three monthly maxima of these data was estimated as \\(0.0333\\). However, this estimate didn’t take account of the dependence, measured via the extremal index \\(\\theta\\), estimated in Figure 7.2. Figure 7.2: Runs estimate of the extremal index for BMW data. A threshold of \\(u=0.03\\) and run length \\(m = 10\\) appears to give a stable estimate of \\(\\hat{\\theta} \\approx 0.593\\). Including this in the VaR\\((0.99)\\) calculation gives \\[ \\hat\\mu -\\frac{\\hat\\sigma}{\\hat\\xi}\\left[1-\\{-0.593 \\times 66 \\log(0.99)\\}^{-\\hat\\xi}\\right] = 0.0406 \\] which is exceeded by a proportion of \\(0.0103\\) of the data, and closer to the estimate according to the GPD approach. 7.3.5 Volatility In Section 6.1.1.1, we saw that whether or not we account for volatility makes a difference to our analysis. Thus far we have calculated VaR as a quantile of the entire series over a long time period, ignoring the local variation, which is however usually present in financial time series (see, for example, Figure 7.1). In practice, we may want to estimate VaR for tomorrow rather than on an arbitrary day. One approach to removing volatility is similar to that seen in Section 6.1.1.1 . Define the filtered negative returns by \\[\\begin{eqnarray*} Z_t = \\frac{X_t - \\tilde{E}(X_t)}{\\tilde{SD}(X_t)} \\end{eqnarray*}\\] where \\(\\tilde{E}(X_t)\\) and \\(\\tilde{SD}(X_t)\\) are local estimates of the mean and standard deviation, for example using the sample mean and sample standard deviation of the previous \\(h\\) observations. Suppose that we have a series of length \\(T\\). The VaR for the day \\(T+1\\) can be estimated as \\[\\begin{eqnarray*} \\mbox{VaR}(p)[T+1] = \\mbox{VaR}_{Z}(p)\\times \\tilde{SD}(X_T) + \\tilde{E}(X_T) \\end{eqnarray*}\\] where \\({\\mbox{VaR}_{Z}(p)}\\) is the VaR estimated from the filtered series \\(Z_t\\). BMW example Figure 7.3: Filtered series for BMW data. We will calculate VaR\\((p)[t]\\) for \\(t\\geq h\\), with \\(h = 20\\). First we obtain the filtered series, which is shown in Figure 7.3. Following the usual procedure the \\(\\text{VaR}(0.99)\\) is estimated as \\(3.06\\). Transforming to the return series gives the following plot of daily \\(99\\)% VaR estimates The proportion of data exceeding \\(\\text{VaR}(0.99)[t]\\) is (interestingly) \\(0.0105\\), while the proportion of data exceeding \\(\\text{VaR}(0.99)\\) is \\(0.0103\\). Both are correct in the long run. 7.4 Volatility models Figure 7.4: Returns of the FTSE index. You may have learned about (G)ARCH and other models that account for the volatility directly. Here, we will be using the GARCH\\((1,1)\\) model as an example for the FTSE return series in Figure 7.4, denoted by \\(\\{X_t\\}\\). The model equations are \\[\\begin{align*} X_t &amp;= \\sigma_t\\epsilon_t, \\\\ \\sigma_t^2 &amp;= \\omega + \\alpha X_{t-1}^2 + \\beta \\sigma_{t-1}^2, \\end{align*}\\] where \\(\\omega&gt;0,\\alpha\\geq0,\\beta\\geq0\\), and the innovation series \\(\\{\\epsilon_t\\}\\) are IID with expectation 0 and variance 1. It is also usually assumed that the innovations are normally distributed. We examine if the normality assumption is reasonable after fitting to the FTSE data. From the Q-Q plot of the residuals, which are proxies of the innovation sequence, we can safely say they are not normally distributed. We can also fit the GPD to high quantiles to the residuals and the negative residuals to examine the behaviour on both tails. According to the parameter stability plots, it could be argued that the residuals (top) have a shape parameter of 0 i.e. the Gumbel distribution, whose DoA contains the normal distribution. On the other hand, it is quite clear that the negative residuals (bottom) have a heavier tail than what would be according to the normal distribution. Ideally, the asymmetry and heaviness of the tails should be incorporated into the GARCH model. As such a unifying model is beyond the scope of this module, we will simply list out a few practical difficulties: Not even the Student’s \\(t\\)-distribution can circumvent the issue with the violation of the normality assumption, because the data are asymmetric in the tails, while the distribution itself is symmetric; Inherently, it is inconsistent to fit the GARCH model assuming normality first, followed by fitting non-normal distribution(s); If we are to fit the GARCH model while assuming the appropriate innovation distribution, we will need a distribution that encompasses not only the two tails but also the body; The two constraints of zero mean &amp; unit variance for the innovations are still required. See So and Chan (2014) for an example of incorporating extreme value methods in GARCH models. 7.5 Further reading What is described in the section above is from a methodological point of view. For more theoretical results on the extremes of volatility models, please see de Haan et al. (1989), Breidt and Davis (1998), Mikosch and Stărică (2000), and Laurini and Tawn (2012). For some more examples of financial applications, see Figueiredo, Gomes, and Henriques-Rodrigues (2017) and Castro-Camilo, de Carvalho, and Wadsworth (2018). Bibliography "],["further.html", "8 Further Reading 8.1 General &amp; books 8.2 Environmental applications", " 8 Further Reading This section contains references on extremes which may not be particularly related to the individual topics in the notes, and/or fall into multiple categories. While they are mostly out of the scope of this module, they are good starting points for pursuing further research in the field. 8.1 General &amp; books Hill (1975) and Pickands (1975), which were published in the same journal and year back in 1975, are two of the papers that paved the way for classical theory of extreme value theory. Geffroy (1958/59) is another old publication on extremes. Introductory books to the topic include J. Beirlant et al. (2004) and S. Coles (2001), the latter of which is accompanied by the R package ismev that you have used in the labs. This paper by Jan Beirlant, Caeiro, and Gomes (2012) lists the open problems (at least they were a few years ago) in univariate extremes. This review by Chavez-Demoulin and Davison (2012) covers various topics introduced in this module, albeit titled “Modelling time series extremes”. 8.2 Environmental applications There are a lot of applications to environmental data, and we list a few areas here: Climate change and/or downscaling: Burke, Perry, and Brown (2010), Mannshardt-Shamseldin et al. (2010), Kallache et al. (2011), Winter, Tawn, and Brown (2016), Shen, Mickley, and Gilleland (2016) Ocean extremes: Jonathan and Ewans (2007), Jonathan and Ewans (2013), Mackay and Jonathan (2020) Flooding: Keef, Tawn, and Svensson (2009), Keef, Tawn, and Lamb (2013) This blog post discusses how we quantify risk from environmental extreme events in a changing world, and contains some additional references. Bibliography "],["appendices.html", "Appendices Formula sheet Asymptotic notation", " Appendices Formula sheet Here is a reminder of some formulae. You may find them helpful for reading the course notes and completing the exercises. The Taylor series expansion of a function \\(f(x)\\) about a point \\(x=a\\): \\[\\begin{eqnarray*} f(x) = f(a) + f&#39;(a)(x-a) + \\frac{f&#39;&#39;(a)}{2!}(x-a)^2 + \\ldots + \\frac{f^{(n)}(a)}{n!}(x-a)^n + \\ldots \\end{eqnarray*}\\] If \\(a = 0\\) then the expansion is known as a Maclaurin series. \\[\\begin{eqnarray*} \\begin{array}{lrcl} \\mbox{Taylor series expanison about 0:}&amp; \\exp(x) &amp;=&amp; \\sum_{n=0}^\\infty \\frac{x^n}{n!}\\\\ \\mbox{Mercator series:} &amp; \\log(1+x) &amp;=&amp; \\sum_{k=1}^\\infty\\frac{(-1)^{k+1}}{k}x^k; -1&lt;x\\leq1 \\\\ &amp; \\exp(x) &amp;=&amp; \\lim_{n\\rightarrow\\infty}\\left(1 + \\frac xn\\right)^n\\\\ &amp; \\frac{1}{1-r} &amp;=&amp;\\sum_{i=1}^\\infty r^{i-1} \\end{array} \\end{eqnarray*}\\] Asymptotic notation Here is some notation that is useful for expressing some of the asymptotic relationships presented in this course. Big-O notation \\(f(n) = O(g(n))\\) means there are positive constants \\(c\\) and \\(k\\), such that \\(0 \\leq |f(n)| \\leq cg(n)\\) for all \\(n\\geq k\\). The values of \\(c\\) and \\(k\\) must be fixed for the function \\(f\\) and must not depend on \\(n\\). Strictly, the character is the upper-case Greek letter Omicron, not the letter O, but who can tell the difference? Informally, saying some equation \\(f(n) = O(g(n))\\) means it is less than some constant multiple of \\(g(n)\\). The notation is read, “f of n is big oh of g of n”. Little-o notation \\(f(n) = o(g(n))\\) means for all \\(c &gt; 0\\), there exists some \\(k &gt; 0\\) such that \\(0 \\leq |f(n)| &lt; cg(n)\\) for all \\(n \\geq k\\). The value of \\(k\\) must not depend on \\(n\\), but may depend on \\(c\\). Strictly, the character is the lower-case Greek letter omicron. Informally, saying some equation \\(f(n) = o(g(n))\\) means \\(f(n)\\) becomes insignificant relative to \\(g(n)\\) as \\(n\\) approaches infinity. The notation is read, “f of n is little o of g of n”. \\(\\sim\\), sim \\(f(x)\\sim g(x)\\) means \\(lim_{x \\rightarrow \\infty} f(x)/g(x) = 1\\). Informally, saying some equation \\(f(n)\\) similar to \\(g(n)\\) means it grows at the same rate as g(n). "],["bibliography.html", "Bibliography", " Bibliography "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
